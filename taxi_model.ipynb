{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unix time: https://www.unixtimestamp.com/\n",
    "import datetime  # Convert to unix time\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time  # Convert to unix time\n",
    "import warnings\n",
    "\n",
    "import dask.dataframe as dd  # similar to pandas\n",
    "import matplotlib.pylab as plt\n",
    "# if numpy is not installed already : pip3 install numpy\n",
    "import numpy as np  # Do aritmetic operations on arrays\n",
    "import pandas as pd  # pandas to create small dataframes\n",
    "import seaborn as sns  # Plots\n",
    "# to install xgboost: pip3 install xgboost\n",
    "# if it didnt happen check install_xgboost.JPG\n",
    "import xgboost as xgb\n",
    "from matplotlib import rcParams  # Size of plots\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans  # Clustering\n",
    "# to install sklearn: pip install -U scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (mean_absolute_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_squared_error, r2_score)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_year = 2018\n",
    "base_month_count = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    months_frame = []\n",
    "    months_groupby = []\n",
    "    for i in range(1,base_month_count+1):\n",
    "        tmp_frame = pd.read_parquet(f'preprocessing_yellow_tripdata_{base_year+1}_{i}.parquet',engine='pyarrow')\n",
    "        tmp_groupby = tmp_frame[['PULocationID','pickup_bins','trip_distance']].groupby(['PULocationID','pickup_bins']).count()\n",
    "       \n",
    "        months_frame.append(tmp_frame)\n",
    "        months_groupby.append(tmp_groupby)\n",
    "    return months_frame, months_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_frame, months_groupby = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>trip_times</th>\n",
       "      <th>pickup_times</th>\n",
       "      <th>Speed</th>\n",
       "      <th>pickup_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.36</td>\n",
       "      <td>4</td>\n",
       "      <td>114</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.016667</td>\n",
       "      <td>1.546237e+09</td>\n",
       "      <td>9.049908</td>\n",
       "      <td>-52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16350</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.92</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>26.30</td>\n",
       "      <td>3.00</td>\n",
       "      <td>29.550000</td>\n",
       "      <td>1.546250e+09</td>\n",
       "      <td>9.989848</td>\n",
       "      <td>-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17826</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>13</td>\n",
       "      <td>231</td>\n",
       "      <td>6.80</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>1.546246e+09</td>\n",
       "      <td>11.529412</td>\n",
       "      <td>-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101380</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>41</td>\n",
       "      <td>74</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>1.546253e+09</td>\n",
       "      <td>8.116364</td>\n",
       "      <td>-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101508</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.26</td>\n",
       "      <td>41</td>\n",
       "      <td>48</td>\n",
       "      <td>19.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.016667</td>\n",
       "      <td>1.546255e+09</td>\n",
       "      <td>21.016648</td>\n",
       "      <td>-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6714646</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.55</td>\n",
       "      <td>262</td>\n",
       "      <td>143</td>\n",
       "      <td>11.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.233333</td>\n",
       "      <td>1.230737e+09</td>\n",
       "      <td>13.620178</td>\n",
       "      <td>-525886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6795253</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>263</td>\n",
       "      <td>168</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.866667</td>\n",
       "      <td>1.546230e+09</td>\n",
       "      <td>17.953368</td>\n",
       "      <td>-63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6796436</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>263</td>\n",
       "      <td>141</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.066667</td>\n",
       "      <td>1.546241e+09</td>\n",
       "      <td>21.375000</td>\n",
       "      <td>-45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6827621</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.016667</td>\n",
       "      <td>1.230741e+09</td>\n",
       "      <td>7.858628</td>\n",
       "      <td>-525878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6827622</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.16</td>\n",
       "      <td>263</td>\n",
       "      <td>237</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.633333</td>\n",
       "      <td>1.230742e+09</td>\n",
       "      <td>8.061776</td>\n",
       "      <td>-525876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       "39                   1.0           1.36             4           114   \n",
       "16350                1.0           4.92            13            48   \n",
       "17826                1.0           0.49            13           231   \n",
       "101380               2.0           0.62            41            74   \n",
       "101508               2.0           5.26            41            48   \n",
       "...                  ...            ...           ...           ...   \n",
       "6714646              1.0           2.55           262           143   \n",
       "6795253              1.0           3.85           263           168   \n",
       "6796436              2.0           0.38           263           141   \n",
       "6827621              1.0           1.05           263           263   \n",
       "6827622              1.0           1.16           263           237   \n",
       "\n",
       "         total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       "39               9.30        0.00    9.016667  1.546237e+09   9.049908   \n",
       "16350           26.30        3.00   29.550000  1.546250e+09   9.989848   \n",
       "17826            6.80        2.00    2.550000  1.546246e+09  11.529412   \n",
       "101380           6.30        0.00    4.583333  1.546253e+09   8.116364   \n",
       "101508          19.30        0.00   15.016667  1.546255e+09  21.016648   \n",
       "...               ...         ...         ...           ...        ...   \n",
       "6714646         11.30        0.00   11.233333  1.230737e+09  13.620178   \n",
       "6795253         14.80        0.00   12.866667  1.546230e+09  17.953368   \n",
       "6796436          5.16        0.86    1.066667  1.546241e+09  21.375000   \n",
       "6827621          7.80        0.00    8.016667  1.230741e+09   7.858628   \n",
       "6827622          8.30        0.00    8.633333  1.230742e+09   8.061776   \n",
       "\n",
       "         pickup_bins  \n",
       "39               -52  \n",
       "16350            -31  \n",
       "17826            -37  \n",
       "101380           -25  \n",
       "101508           -23  \n",
       "...              ...  \n",
       "6714646      -525886  \n",
       "6795253          -63  \n",
       "6796436          -45  \n",
       "6827621      -525878  \n",
       "6827622      -525876  \n",
       "\n",
       "[249 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "months_frame[0][months_frame[0]['pickup_bins']<0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴욕 지역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_df = pd.read_csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"Manhattan\"\n",
    "nyc_region = taxi_zone_df[taxi_zone_df['Borough'] == region]\n",
    "nyc_region_number = nyc_region['LocationID']\n",
    "nyc_regions_cnt = len(nyc_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[         passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    1.0           3.80             4           246   \n",
       " 1                    1.0           2.70             4            87   \n",
       " 2                    1.0           2.30             4           233   \n",
       " 3                    1.0           4.70             4           263   \n",
       " 4                    1.0           3.90             4           261   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 6950960              NaN           6.26           263           232   \n",
       " 6950961              NaN          15.34           263            98   \n",
       " 6950962              NaN           9.16           263            32   \n",
       " 6950963              NaN          18.53           263           191   \n",
       " 6950964              NaN           4.43           263           159   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0               26.30        0.00   40.583333  1.546269e+09   5.618070   \n",
       " 1               13.30        1.00    9.583333  1.546269e+09  16.904348   \n",
       " 2               11.75        1.95    9.066667  1.546270e+09  15.220588   \n",
       " 3               20.15        3.35   13.900000  1.546269e+09  20.287770   \n",
       " 4               17.15        2.85    9.350000  1.546271e+09  25.026738   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 6950960         36.00        0.00   30.000000  1.548916e+09  12.520000   \n",
       " 6950961         62.70        0.00   43.000000  1.548919e+09  21.404651   \n",
       " 6950962         46.50        0.00   28.000000  1.548926e+09  19.628571   \n",
       " 6950963         76.00        0.00   38.400000  1.548928e+09  28.953125   \n",
       " 6950964         29.00        0.00   15.633333  1.548937e+09  17.002132   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  1  \n",
       " 1                  1  \n",
       " 2                  2  \n",
       " 3                  1  \n",
       " 4                  4  \n",
       " ...              ...  \n",
       " 6950960         4412  \n",
       " 6950961         4418  \n",
       " 6950962         4429  \n",
       " 6950963         4431  \n",
       " 6950964         4447  \n",
       " \n",
       " [6870890 rows x 10 columns],\n",
       "          passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    1.0           1.20             4           125   \n",
       " 1                    1.0           7.00             4           260   \n",
       " 2                    1.0           1.40             4            45   \n",
       " 3                    1.0           1.41             4           114   \n",
       " 4                    1.0           1.30             4           125   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 6423119              NaN          18.64           263           191   \n",
       " 6423120              NaN           3.35           263            48   \n",
       " 6423121              NaN           9.19           263            32   \n",
       " 6423122              NaN           4.25           263           263   \n",
       " 6423123              NaN          11.58           263           240   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0                8.30        0.00    7.433333  1.548948e+09   9.686099   \n",
       " 1               31.30        4.00   29.516667  1.548949e+09  14.229249   \n",
       " 2               11.80        2.50    9.450000  1.548950e+09   8.888889   \n",
       " 3               11.76        1.96   10.400000  1.548948e+09   8.134615   \n",
       " 4                9.80        1.00    7.483333  1.548947e+09  10.423163   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 6423119         76.00        0.00   71.000000  1.551343e+09  15.752113   \n",
       " 6423120         29.00        0.00   26.000000  1.551343e+09   7.730769   \n",
       " 6423121         46.50        0.00   36.716667  1.551345e+09  15.017703   \n",
       " 6423122         29.00        0.00   20.200000  1.551350e+09  12.623762   \n",
       " 6423123         51.90        0.00   23.000000  1.551352e+09  30.208696   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  2  \n",
       " 1                  3  \n",
       " 2                  4  \n",
       " 3                  1  \n",
       " 4                  0  \n",
       " ...              ...  \n",
       " 6423119         3994  \n",
       " 6423120         3992  \n",
       " 6423121         3997  \n",
       " 6423122         4005  \n",
       " 6423123         4008  \n",
       " \n",
       " [6344056 rows x 10 columns],\n",
       "          passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    1.0           2.40             4           164   \n",
       " 1                    1.0           5.26             4            52   \n",
       " 2                    1.0           1.80             4           249   \n",
       " 3                    2.0           2.99             4           161   \n",
       " 4                    1.0           0.78             4            79   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 7142387              NaN           7.61           263           129   \n",
       " 7142388              NaN          10.72           263            92   \n",
       " 7142389              NaN           6.92           263           169   \n",
       " 7142390              NaN           3.89           263           100   \n",
       " 7142391              NaN           9.27           263            31   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0               16.80        2.00   13.083333  1.551369e+09  11.006369   \n",
       " 1               20.80        0.00   14.066667  1.551367e+09  22.436019   \n",
       " 2               15.95        2.65   11.900000  1.551367e+09   9.075630   \n",
       " 3               18.96        3.16   13.216667  1.551368e+09  13.573770   \n",
       " 4               11.16        1.86    4.883333  1.551368e+09   9.583618   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 7142387         39.50        0.00   29.000000  1.553939e+09  15.744828   \n",
       " 7142388         49.20        0.00   18.383333  1.553975e+09  34.988214   \n",
       " 7142389         36.00        0.00   17.550000  1.553989e+09  23.658120   \n",
       " 7142390         29.00        0.00   25.000000  1.554002e+09   9.336000   \n",
       " 7142391         46.50        0.00   26.000000  1.554012e+09  21.392308   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  4  \n",
       " 1                  1  \n",
       " 2                  0  \n",
       " 3                  3  \n",
       " 4                  3  \n",
       " ...              ...  \n",
       " 7142387         4287  \n",
       " 7142388         4348  \n",
       " 7142389         4370  \n",
       " 7142390         4393  \n",
       " 7142391         4409  \n",
       " \n",
       " [7052173 rows x 10 columns],\n",
       "          passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    1.0           3.10             4            88   \n",
       " 1                    2.0           2.15             4           233   \n",
       " 2                    1.0           0.70             4            79   \n",
       " 3                    1.0           2.46             4           233   \n",
       " 4                    1.0           2.75             4            87   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 6770377              NaN           4.51           263            50   \n",
       " 6770378              NaN           3.09           263           168   \n",
       " 6770379              NaN           6.86           263           169   \n",
       " 6770380              NaN           8.79           263            31   \n",
       " 6770381              NaN           5.81           263           232   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0               15.80        0.00   11.483333  1.554045e+09  16.197388   \n",
       " 1               14.00        1.70    8.200000  1.554047e+09  15.731707   \n",
       " 2                8.80        0.00    3.883333  1.554046e+09  10.815451   \n",
       " 3               15.36        2.56    7.183333  1.554045e+09  20.547564   \n",
       " 4               14.30        0.00    7.100000  1.554045e+09  23.239437   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 6770377         25.50        0.00   26.000000  1.556614e+09  10.407692   \n",
       " 6770378         25.50        0.00   23.000000  1.556613e+09   8.060870   \n",
       " 6770379         39.50        0.00   25.150000  1.556615e+09  16.365805   \n",
       " 6770380         46.50        0.00   19.783333  1.556616e+09  26.658804   \n",
       " 6770381         32.50        0.00   15.000000  1.556621e+09  23.240000   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  1  \n",
       " 1                  3  \n",
       " 2                  3  \n",
       " 3                  1  \n",
       " 4                  0  \n",
       " ...              ...  \n",
       " 6770377         4282  \n",
       " 6770378         4280  \n",
       " 6770379         4284  \n",
       " 6770380         4286  \n",
       " 6770381         4294  \n",
       " \n",
       " [6685807 rows x 10 columns],\n",
       "          passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    1.0           9.37             4            35   \n",
       " 1                    2.0           3.10             4           161   \n",
       " 2                    1.0           1.00             4           107   \n",
       " 3                    2.0           1.11             4           148   \n",
       " 4                    1.0           1.90             4           170   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 6873857              NaN           4.73           263           113   \n",
       " 6873858              NaN           9.64           263           200   \n",
       " 6873859              NaN           8.62           263            25   \n",
       " 6873860              NaN           2.98           263           143   \n",
       " 6873862              NaN          16.78           263            39   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0               32.80        0.00   28.966667  1.556639e+09  19.408516   \n",
       " 1               20.15        3.35   14.066667  1.556639e+09  13.222749   \n",
       " 2               12.35        2.05    7.450000  1.556638e+09   8.053691   \n",
       " 3               11.76        1.96    5.300000  1.556637e+09  12.566038   \n",
       " 4               14.15        2.35    7.466667  1.556640e+09  15.267857   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 6873857         29.00        0.00   31.000000  1.559281e+09   9.154839   \n",
       " 6873858         49.20        0.00   40.000000  1.559281e+09  14.460000   \n",
       " 6873859         43.00        0.00   25.000000  1.559280e+09  20.688000   \n",
       " 6873860         25.50        0.00   22.050000  1.559293e+09   8.108844   \n",
       " 6873862         81.00        0.00   52.650000  1.559300e+09  19.122507   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  3  \n",
       " 1                  4  \n",
       " 2                  3  \n",
       " 3                  1  \n",
       " 4                  5  \n",
       " ...              ...  \n",
       " 6873857         4407  \n",
       " 6873858         4408  \n",
       " 6873859         4405  \n",
       " 6873860         4428  \n",
       " 6873862         4439  \n",
       " \n",
       " [6781464 rows x 10 columns],\n",
       "          passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    1.0           0.83             4           113   \n",
       " 1                    1.0           0.99             4           148   \n",
       " 2                    1.0           1.80             4            90   \n",
       " 3                    3.0           4.05             4            65   \n",
       " 4                    2.0           1.43             4           249   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 6282043              NaN           5.20           263            68   \n",
       " 6282044              NaN           4.82           263             4   \n",
       " 6282045              NaN          10.70           263            95   \n",
       " 6282046              NaN           3.20           263            42   \n",
       " 6282047              NaN           8.19           263           136   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0               11.76        1.96    6.650000  1.559318e+09   7.488722   \n",
       " 1               12.30        1.00    9.183333  1.559314e+09   6.468240   \n",
       " 2               16.55        2.75   11.750000  1.559316e+09   9.191489   \n",
       " 3               20.80        1.50   16.583333  1.559318e+09  14.653266   \n",
       " 4               14.76        2.46   10.800000  1.559316e+09   7.944444   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 6282043         32.50        0.00   23.133333  1.561854e+09  13.487032   \n",
       " 6282044         25.74        0.00   19.000000  1.561864e+09  15.221053   \n",
       " 6282045         39.26        0.00   24.000000  1.561876e+09  26.750000   \n",
       " 6282046         26.41        0.00   15.000000  1.561877e+09  12.800000   \n",
       " 6282047         43.00        0.00   25.766667  1.561890e+09  19.071151   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  5  \n",
       " 1                  0  \n",
       " 2                  2  \n",
       " 3                  4  \n",
       " 4                  2  \n",
       " ...              ...  \n",
       " 6282043         4232  \n",
       " 6282044         4248  \n",
       " 6282045         4268  \n",
       " 6282046         4270  \n",
       " 6282047         4292  \n",
       " \n",
       " [6188574 rows x 10 columns],\n",
       "          passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    1.0           1.00             4           114   \n",
       " 1                    2.0           5.97             4           239   \n",
       " 2                    1.0           4.00             4           230   \n",
       " 3                    1.0           2.80             4            88   \n",
       " 4                    1.0           3.50             4           255   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 5678283              NaN           2.67           263           239   \n",
       " 5678284              NaN           2.51           263           151   \n",
       " 5678285              NaN           3.52           263           168   \n",
       " 5678286              NaN           3.52           263            74   \n",
       " 5678287              NaN          18.54           263            91   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0               10.30        0.00    7.950000  1.561909e+09   7.547170   \n",
       " 1               29.16        4.86   20.600000  1.561909e+09  17.388350   \n",
       " 2               20.80        0.00   22.083333  1.561908e+09  10.867925   \n",
       " 3               13.80        0.00    6.666667  1.561909e+09  25.200000   \n",
       " 4               22.45        5.15   15.100000  1.561910e+09  13.907285   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 5678283         22.00        0.00   15.766667  1.564543e+09  10.160677   \n",
       " 5678284         33.51        0.00   15.000000  1.564544e+09  10.040000   \n",
       " 5678285         27.37        0.00   18.766667  1.564550e+09  11.253996   \n",
       " 5678286         26.41        0.00   22.316667  1.564550e+09   9.463779   \n",
       " 5678287         68.10        0.00   80.000000  1.564559e+09  13.905000   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  4  \n",
       " 1                  2  \n",
       " 2                  1  \n",
       " 3                  3  \n",
       " 4                  4  \n",
       " ...              ...  \n",
       " 5678283         4393  \n",
       " 5678284         4395  \n",
       " 5678285         4406  \n",
       " 5678286         4405  \n",
       " 5678287         4420  \n",
       " \n",
       " [5589147 rows x 10 columns],\n",
       "          passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    1.0           0.89             4            79   \n",
       " 1                    1.0           1.82             4           144   \n",
       " 2                    1.0           3.15             4           229   \n",
       " 3                    6.0          12.20             4            94   \n",
       " 4                    3.0           4.44             4           112   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 5443681              NaN          15.35           263           197   \n",
       " 5443682              NaN           4.65           263            42   \n",
       " 5443683              NaN           3.66           263            50   \n",
       " 5443684              NaN           7.64           263           169   \n",
       " 5443685              NaN          11.22           263           135   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0               10.23        0.93    4.500000  1.564586e+09  11.866667   \n",
       " 1               16.56        2.76   12.466667  1.564587e+09   8.759358   \n",
       " 2               15.30        0.00    9.183333  1.564586e+09  20.580762   \n",
       " 3               39.80        0.00   28.150000  1.564585e+09  26.003552   \n",
       " 4               24.36        4.06   15.483333  1.564587e+09  17.205597   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 5443681         45.61        0.00   43.000000  1.567227e+09  21.418605   \n",
       " 5443682         26.41        0.00   13.000000  1.567230e+09  21.461538   \n",
       " 5443683         28.01        0.00   21.116667  1.567241e+09  10.399369   \n",
       " 5443684         32.94        0.00   38.683333  1.567242e+09  11.850065   \n",
       " 5443685         44.43        0.00   37.000000  1.567242e+09  18.194595   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  2  \n",
       " 1                  3  \n",
       " 2                  2  \n",
       " 3                  0  \n",
       " 4                  3  \n",
       " ...              ...  \n",
       " 5443681         4403  \n",
       " 5443682         4408  \n",
       " 5443683         4427  \n",
       " 5443684         4427  \n",
       " 5443685         4428  \n",
       " \n",
       " [5354462 rows x 10 columns],\n",
       "          passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    1.0           1.50             4           113   \n",
       " 1                    1.0           2.64             4           186   \n",
       " 2                    1.0           2.92             4           256   \n",
       " 3                    1.0           3.00             4           161   \n",
       " 4                    1.0           0.80             4           232   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 5934465              NaN           3.90           263            48   \n",
       " 5934466              NaN           0.83           263           236   \n",
       " 5934467              NaN           3.63           263           186   \n",
       " 5934468              NaN           0.75           263           262   \n",
       " 5934469              NaN           9.04           263            95   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0               10.30        0.00    6.783333  1.567264e+09  13.267813   \n",
       " 1               15.30        0.00   13.666667  1.567266e+09  11.590244   \n",
       " 2               21.36        3.56   15.916667  1.567265e+09  11.007330   \n",
       " 3               14.80        0.00   10.233333  1.567265e+09  17.589577   \n",
       " 4               12.95        2.15    8.533333  1.567267e+09   5.625000   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 5934465         28.01        0.00   25.950000  1.569823e+09   9.017341   \n",
       " 5934466         28.01        0.00    6.000000  1.569825e+09   8.300000   \n",
       " 5934467         29.00        0.00   26.000000  1.569826e+09   8.376923   \n",
       " 5934468         39.26        0.00    5.866667  1.569846e+09   7.670455   \n",
       " 5934469         39.26        0.00   24.600000  1.569848e+09  22.048780   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  0  \n",
       " 1                  4  \n",
       " 2                  2  \n",
       " 3                  2  \n",
       " 4                  5  \n",
       " ...              ...  \n",
       " 5934465         4266  \n",
       " 5934466         4268  \n",
       " 5934467         4270  \n",
       " 5934468         4304  \n",
       " 5934469         4308  \n",
       " \n",
       " [5838570 rows x 10 columns],\n",
       "          passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    4.0           0.50             4            79   \n",
       " 1                    1.0           5.56             4             7   \n",
       " 2                    6.0           1.36             4           148   \n",
       " 3                    1.0           6.49             4             7   \n",
       " 4                    1.0          12.60             4           169   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 6553294              NaN          14.92           263            14   \n",
       " 6553295              NaN           6.68           263           263   \n",
       " 6553296              NaN           7.38           263            70   \n",
       " 6553297              NaN          11.48           263           185   \n",
       " 6553298              NaN           9.76           263            78   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0                9.95        1.65    3.483333  1.569858e+09   8.612440   \n",
       " 1               25.00        2.20   19.483333  1.569856e+09  17.122327   \n",
       " 2               12.30        0.00   10.150000  1.569856e+09   8.039409   \n",
       " 3               34.12        6.82   27.016667  1.569858e+09  14.413325   \n",
       " 4               41.80        0.00   27.500000  1.569857e+09  27.490909   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 6553294         53.84        0.00   41.000000  1.572499e+09  21.834146   \n",
       " 6553295         32.34        0.00   24.000000  1.572501e+09  16.700000   \n",
       " 6553296         34.45        0.00   25.216667  1.572506e+09  17.559815   \n",
       " 6553297         46.50        0.00   32.583333  1.572513e+09  21.139642   \n",
       " 6553298         32.94        0.00   24.000000  1.572521e+09  24.400000   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  4  \n",
       " 1                  0  \n",
       " 2                  0  \n",
       " 3                  4  \n",
       " 4                  2  \n",
       " ...              ...  \n",
       " 6553294         4404  \n",
       " 6553295         4409  \n",
       " 6553296         4416  \n",
       " 6553297         4429  \n",
       " 6553298         4442  \n",
       " \n",
       " [6452680 rows x 10 columns],\n",
       "          passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    4.0           0.90             4            79   \n",
       " 1                    2.0           1.83             4           249   \n",
       " 2                    1.0           1.27             4           144   \n",
       " 3                    1.0           2.80             4            87   \n",
       " 4                    5.0           6.40             4           145   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 6281181              NaN           3.10           263            42   \n",
       " 6281182              NaN          15.05           263            14   \n",
       " 6281183              NaN           0.43           263           236   \n",
       " 6281184              NaN           6.79           263           235   \n",
       " 6281186              NaN           9.11           263           231   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0               10.30         0.0    8.533333  1.572536e+09   6.328125   \n",
       " 1               13.80         0.0   13.133333  1.572537e+09   8.360406   \n",
       " 2               11.80         0.0    9.300000  1.572537e+09   8.193548   \n",
       " 3               15.80         1.0    8.933333  1.572536e+09  18.805970   \n",
       " 4               26.30         2.0   20.950000  1.572536e+09  18.329356   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 6281181         26.41         0.0   16.000000  1.575077e+09  11.625000   \n",
       " 6281182         53.84         0.0   31.000000  1.575090e+09  29.129032   \n",
       " 6281183         19.63         0.0    4.000000  1.575087e+09   6.450000   \n",
       " 6281184         32.94         0.0   20.300000  1.575094e+09  20.068966   \n",
       " 6281186         32.34         0.0   21.000000  1.575104e+09  26.028571   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  3  \n",
       " 1                  5  \n",
       " 2                  4  \n",
       " 3                  3  \n",
       " 4                  3  \n",
       " ...              ...  \n",
       " 6281181         4238  \n",
       " 6281182         4259  \n",
       " 6281183         4255  \n",
       " 6281184         4265  \n",
       " 6281186         4282  \n",
       " \n",
       " [6177143 rows x 10 columns],\n",
       "          passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       " 0                    1.0           2.71             4           256   \n",
       " 1                    1.0           0.11             4           232   \n",
       " 2                    1.0           3.77             4           141   \n",
       " 3                    1.0           3.10             4           256   \n",
       " 4                    1.0           8.08             4           194   \n",
       " ...                  ...            ...           ...           ...   \n",
       " 6273728              NaN          10.10           263            56   \n",
       " 6273729              NaN          10.50           263           185   \n",
       " 6273730              NaN          10.42           263           220   \n",
       " 6273731              NaN           1.62           263            74   \n",
       " 6273732              NaN           3.54           263            50   \n",
       " \n",
       "          total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       " 0               19.56        3.26   13.966667  1.575127e+09  11.642005   \n",
       " 1                7.30        1.00    0.766667  1.575129e+09   8.608696   \n",
       " 2               21.23        1.93   20.083333  1.575126e+09  11.263071   \n",
       " 3               17.80        0.00   16.300000  1.575127e+09  11.411043   \n",
       " 4               33.92        0.00   16.450000  1.575127e+09  29.471125   \n",
       " ...               ...         ...         ...           ...        ...   \n",
       " 6273728         34.85        0.00   30.466667  1.577776e+09  19.890591   \n",
       " 6273729         46.50        0.00   35.000000  1.577777e+09  18.000000   \n",
       " 6273730         46.50        0.00   26.016667  1.577781e+09  24.030750   \n",
       " 6273731         23.70        0.00    8.400000  1.577784e+09  11.571429   \n",
       " 6273732         28.01        0.00   21.000000  1.577783e+09  10.114286   \n",
       " \n",
       "          pickup_bins  \n",
       " 0                  2  \n",
       " 1                  4  \n",
       " 2                  0  \n",
       " 3                  2  \n",
       " 4                  1  \n",
       " ...              ...  \n",
       " 6273728         4416  \n",
       " 6273729         4417  \n",
       " 6273730         4425  \n",
       " 6273731         4429  \n",
       " 6273732         4428  \n",
       " \n",
       " [6165522 rows x 10 columns]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "months_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464\n"
     ]
    }
   ],
   "source": [
    "# number of 10min indices for jan 2019= 24*31*60/10 = max_pickup_bins_len\n",
    "interval = 10\n",
    "days = [31,28,31,30,31,30,31,31,30,31,30,31]\n",
    "pickup_bins_len = []\n",
    "\n",
    "for day in days:\n",
    "    pickup_bins_len.append(int(24*60*day/interval))\n",
    "max_pickup_bins_len = max(pickup_bins_len)\n",
    "print(max_pickup_bins_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills a value of zero for every bin where no pickup data is present \n",
    "# the count_values: number pickps that are happened in each region for each 10min intravel\n",
    "# there wont be any value if there are no picksups.\n",
    "# values: number of unique bins\n",
    "\n",
    "# for every 10min intravel(pickup_bin) we will check it is there in our unique bin,\n",
    "# if it is there we will add the count_values[index] to smoothed data\n",
    "# if not we add smoothed data (which is calculated based on the methods that are discussed in the above markdown cell)\n",
    "# we finally return smoothed data\n",
    "def smoothing(count_values,values):\n",
    "    smoothed_regions=[] # stores list of final smoothed values of each reigion\n",
    "    ind=0\n",
    "    repeat=0 \n",
    "    smoothed_value=0\n",
    "    for r in range(1,nyc_regions_cnt+1):\n",
    "        smoothed_bins=[] #stores the final smoothed values\n",
    "        repeat=0\n",
    "        for i in range(max_pickup_bins_len):\n",
    "            if repeat!=0: # prevents iteration for a value which is already visited/resolved\n",
    "                repeat-=1\n",
    "                continue\n",
    "            if i in values[r-1]: #checks if the pickup-bin exists \n",
    "                smoothed_bins.append(count_values[ind-1]) # appends the value of the pickup bin if it exists\n",
    "            else:\n",
    "                if i!=0:\n",
    "                    right_hand_limit=0\n",
    "                    for j in range(i,max_pickup_bins_len):\n",
    "                        if  j not in values[r-1]: #searches for the left-limit or the pickup-bin value which has a pickup value\n",
    "                            continue\n",
    "                        else:\n",
    "                            right_hand_limit=j\n",
    "                            break\n",
    "                    if right_hand_limit==0:\n",
    "                    #Case 1: When we have the last/last few values are found to be missing,hence we have no right-limit here\n",
    "                        smoothed_value=count_values[ind-1]*1.0/((max_pickup_bins_len-1-i)+2)*1.0                               \n",
    "                        for j in range(i,max_pickup_bins_len):                              \n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n",
    "                        repeat=(max_pickup_bins_len-1-i)\n",
    "                        ind-=1\n",
    "                    else:\n",
    "                    #Case 2: When we have the missing values between two known values\n",
    "                        smoothed_value=(count_values[ind-1]+count_values[ind])*1.0/((right_hand_limit-i)+2)*1.0             \n",
    "                        for j in range(i,right_hand_limit+1):\n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n",
    "                        repeat=(right_hand_limit-i)\n",
    "                else:\n",
    "                    #Case 3: When we have the first/first few values are found to be missing,hence we have no left-limit here\n",
    "                    right_hand_limit=0\n",
    "                    for j in range(i,max_pickup_bins_len):\n",
    "                        if  j not in values[r-1]:\n",
    "                            continue\n",
    "                        else:\n",
    "                            right_hand_limit=j\n",
    "                            break\n",
    "                    smoothed_value=count_values[ind]*1.0/((right_hand_limit-i)+1)*1.0\n",
    "                    for j in range(i,right_hand_limit+1):\n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                    repeat=(right_hand_limit-i)\n",
    "            ind+=1\n",
    "        smoothed_regions.extend(smoothed_bins)\n",
    "    return smoothed_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unq_pickup_bins(frame):\n",
    "    values = []\n",
    "    for i in nyc_region_number.values:\n",
    "    # for i in range(1,266):\n",
    "        new = frame[frame['PULocationID'] == i]\n",
    "        list_unq = list(set(new['pickup_bins']))\n",
    "        list_unq.sort()\n",
    "        values.append(list_unq)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_unique = []\n",
    "for frame in months_frame:\n",
    "    months_unique.append(return_unq_pickup_bins(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "52560\n"
     ]
    }
   ],
   "source": [
    "months_smooth = []\n",
    "for groupby, unique in zip(months_groupby, months_unique):\n",
    "    # smoothing을 할 것인가 filling을 할것인가\n",
    "    months_smooth.append(smoothing(groupby['trip_distance'].values,unique))\n",
    "\n",
    "# Making list of all the values of pickup data in every bin for a period of 3 months and storing them region-wise \n",
    "regions_cum = []\n",
    "\n",
    "\n",
    "# number of 10min indices for jan 2019= 24*31*60/10 = 4464      # pickup_bins_len[0]\n",
    "# number of 10min indices for jan 2020 = 24*31*60/10 = 4464     # pickup_bins_len[0]\n",
    "# number of 10min indices for feb 2020 = 24*29*60/10 = 4176     # pickup_bins_len[1]\n",
    "# number of 10min indices for march 2020 = 24*31*60/10 = 4464   # pickup_bins_len[2]\n",
    "# regions_cum: it will contain 40 lists, each list will contain 4464+4176+4464 values which represents the number of pickups \n",
    "# that are happened for three months in 2016 data\n",
    "\n",
    "# nyc_regions_cnt개의 맨허튼 지역\n",
    "for i in range(1,nyc_regions_cnt+1):\n",
    "    cum = []\n",
    "    for index, smooth in enumerate(months_smooth):\n",
    "        cum += smooth[pickup_bins_len[index]*(i-1):pickup_bins_len[index]*i]\n",
    "    regions_cum.append(cum)\n",
    "\n",
    "print(len(regions_cum))\n",
    "print(len(regions_cum[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing data to be split into train and test, The below prepares data in cumulative form which will be later split into test and train\n",
    "# number of 10min indices for jan 2019= 24*31*60/10 = 4464      # pickup_bins_len[0]\n",
    "# number of 10min indices for jan 2020 = 24*31*60/10 = 4464     # pickup_bins_len[0]\n",
    "# number of 10min indices for feb 2020 = 24*29*60/10 = 4176     # pickup_bins_len[1]\n",
    "# number of 10min indices for march 2020 = 24*31*60/10 = 4464   # pickup_bins_len[2]\n",
    "# regions_cum: it will contain 40 lists, each list will contain 4464+4176+4464 values which represents the number of pickups \n",
    "# that are happened for three months in 2016 data\n",
    "\n",
    "# print(len(regions_cum))\n",
    "# 265\n",
    "# print(len(regions_cum[0]))\n",
    "# 4368\n",
    "\n",
    "\n",
    "# we take number of pickups that are happened in last 5 intravels\n",
    "number_of_time_stamps = 5\n",
    "\n",
    "# output varaible\n",
    "# it is list of lists\n",
    "# it will contain number of pickups 4368 for each cluster\n",
    "# len(regions_cum[0]) == 4368\n",
    "output = []\n",
    "\n",
    "sum(pickup_bins_len[:base_month_count])\n",
    "# 우리 데이터\n",
    "# len(regions_cum[0]) - 5(:= # of colunms)\n",
    "# 4368 - 5 = 4363\n",
    "# 13104 - 5 = 13099\n",
    "\n",
    "# tsne_lat will contain 13104-5=13099 times lattitude of cluster center for every cluster\n",
    "# Ex: [[cent_lat 13099times],[cent_lat 13099times], [cent_lat 13099times].... 40 lists]\n",
    "# it is list of lists\n",
    "# tsne_lat = []\n",
    "\n",
    "# tsne_lon will contain 13104-5=13099 times logitude of cluster center for every cluster\n",
    "# Ex: [[cent_long 13099times],[cent_long 13099times], [cent_long 13099times].... 40 lists]\n",
    "# it is list of lists\n",
    "# tsne_lon = []\n",
    "\n",
    "# 우리는 lat, lon 대신에 목적지 ID (PULocationID: 출발지, DOLocationID: 도착지)를 사용할 것이다.\n",
    "tsne_PULocationID = []\n",
    "\n",
    "# we will code each day \n",
    "# sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5, sat=6\n",
    "# for every cluster we will be adding 13099 values, each value represent to which day of the week that pickup bin belongs to\n",
    "# it is list of lists\n",
    "tsne_weekday = []\n",
    "\n",
    "# its an numbpy array, of shape (523960, 5)\n",
    "# each row corresponds to an entry in out data\n",
    "# for the first row we will have [f0,f1,f2,f3,f4] fi=number of pickups happened in i+1th 10min intravel(bin)\n",
    "# the second row will have [f1,f2,f3,f4,f5]\n",
    "# the third row will have [f2,f3,f4,f5,f6]\n",
    "# and so on...\n",
    "tsne_feature = []\n",
    "\n",
    "\n",
    "tsne_feature = [0]*number_of_time_stamps\n",
    "for i in range(1,nyc_regions_cnt+1):\n",
    "    # tsne_lat.append([kmeans.cluster_centers_[i][0]]*13099) # kmeans.cluster_centers_[i][0] := Coordinates of cluster centers. 클러스트 센터의 상관계수\n",
    "    # tsne_lon.append([kmeans.cluster_centers_[i][1]]*13099)\n",
    "\n",
    "    # tsne_PULocationID\n",
    "    tsne_PULocationID.append([i]*(len(regions_cum[0]) - 5))\n",
    "\n",
    "\n",
    "    day_of_the_week_dict = {2015: 4, 2016: 5, 2017: 0, 2018:1, 2019:2, 2020:3, 2021:5, 2022:6}\n",
    "    # jan 1st 2016 is thursday, so we start our day from 4: \"(int(k/144))%7+4\"\n",
    "    # our prediction start from 5th 10min intravel since we need to have number of pickups that are happened in last 5 pickup bins\n",
    "    \n",
    "    # jan 1st 2020 is tue -> 3\n",
    "    tsne_weekday.append([int(((int(k/144))%7+day_of_the_week_dict[base_year+1])%7) for k in range(5,sum(pickup_bins_len[:base_month_count]))])\n",
    "\n",
    "    # jan 1st 2021 is fri -> 5\n",
    "    # tsne_weekday.append([int(((int(k/144))%7+5)%7) for k in range(5,sum(pickup_bins_len[:3]))])\n",
    "    # regions_cum is a list of lists [[x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], .. 40 lsits]\n",
    "    \n",
    "    # 우리 데이터 \n",
    "    # regions_cum [[x_1,x_2,...,x_{len(regions_cum[0]) - 5}],...265 lists] len(regions_cum[0]) - 5 = 4381\n",
    "    tsne_feature = np.vstack((tsne_feature, [regions_cum[i-1][r:r+number_of_time_stamps] for r in range(0,len(regions_cum[i-1])-number_of_time_stamps)]))\n",
    "\n",
    "    output.append(regions_cum[i-1][5:])\n",
    "\n",
    "tsne_feature = tsne_feature[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3626295\n",
      "3626295\n",
      "3626295\n",
      "3626295\n",
      "3626295\n"
     ]
    }
   ],
   "source": [
    "print(tsne_feature.shape[0])\n",
    "print(len(tsne_weekday)*len(tsne_weekday[0]))\n",
    "print(len(output)*len(output[0]))\n",
    "print(nyc_regions_cnt*(len(regions_cum[0])-5))\n",
    "print(len(tsne_PULocationID)*len(tsne_PULocationID[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions of exponential moving averages to be used as a feature in cumulative form\n",
    "\n",
    "# upto now we computed 8 features for every data point that starts from 50th min of the day\n",
    "# 1. cluster center lattitude\n",
    "# 2. cluster center longitude\n",
    "# 3. day of the week \n",
    "# 4. f_t_1: number of pickups that are happened previous t-1th 10min intravel\n",
    "# 5. f_t_2: number of pickups that are happened previous t-2th 10min intravel\n",
    "# 6. f_t_3: number of pickups that are happened previous t-3th 10min intravel\n",
    "# 7. f_t_4: number of pickups that are happened previous t-4th 10min intravel\n",
    "# 8. f_t_5: number of pickups that are happened previous t-5th 10min intravel\n",
    "\n",
    "# from the baseline models we said the exponential weighted moving avarage gives us the best error\n",
    "# we will try to add the same exponential weighted moving avarage at t as a feature to our data\n",
    "# exponential weighted moving avarage => p'(t) = alpha*p'(t-1) + (1-alpha)*P(t-1) \n",
    "alpha=0.3\n",
    "\n",
    "# it is a temporary array that store exponential weighted moving avarage for each 10min intravel, \n",
    "# for each cluster it will get reset\n",
    "# for every cluster it contains 13104 values\n",
    "predicted_values=[]\n",
    "\n",
    "# it is similar like tsne_lat\n",
    "# it is list of lists\n",
    "# predict_list is a list of lists [[x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], .. 40 lsits]\n",
    "predict_list = []\n",
    "tsne_flat_exp_avg = []\n",
    "for r in range(1,nyc_regions_cnt+1):\n",
    "    for i in range(0,len(regions_cum[0])):\n",
    "        if i==0:\n",
    "            predicted_value= regions_cum[r-1][0]\n",
    "            predicted_values.append(0)\n",
    "            continue\n",
    "        predicted_values.append(predicted_value)\n",
    "        predicted_value =int((alpha*predicted_value) + (1-alpha)*(regions_cum[r-1][i]))\n",
    "    predict_list.append(predicted_values[5:])\n",
    "    predicted_values=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52555"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_list[7][27348]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data : 36788\n",
      "size of test data : 15766\n"
     ]
    }
   ],
   "source": [
    "# train, test split : 70% 30% split\n",
    "# Before we start predictions using the tree based regression models we take 3 months of 2016 pickup data \n",
    "# and split it such that for every region we have 70% data in train and 30% in test,\n",
    "# ordered date-wise for every region\n",
    "\n",
    "sizeof_train_data = int((len(regions_cum[0])-5)*0.7)\n",
    "sizeof_test_data = int((len(regions_cum[0])-5)*0.3)\n",
    "\n",
    "\n",
    "print(\"size of train data :\", sizeof_train_data)\n",
    "print(\"size of test data :\", sizeof_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting first 91nyc_regions_cnt timestamp values i.e 70% of 13099 (total timestamps) for our training data\n",
    "train_features =  [tsne_feature[i*(len(regions_cum[0])-5):((len(regions_cum[0])-5)*i+sizeof_train_data)] for i in range(0,nyc_regions_cnt)]\n",
    "\n",
    "test_features = [tsne_feature[((len(regions_cum[0])-5)*(i))+sizeof_train_data:(len(regions_cum[0])-5)*(i+1)] for i in range(0,nyc_regions_cnt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data clusters 69 Number of data points in trian data 36788 Each data point contains 5 features\n",
      "Number of data clusters 69 Number of data points in test data 15767 Each data point contains 5 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data clusters\",len(train_features), \"Number of data points in trian data\", len(train_features[0]), \"Each data point contains\", len(train_features[0][0]),\"features\")\n",
    "print(\"Number of data clusters\",len(train_features), \"Number of data points in test data\", len(test_features[0]), \"Each data point contains\", len(test_features[0][0]),\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting first sizeof_train_data timestamp values i.e 70% of 13099 (total timestamps) for our training data\n",
    "\n",
    "tsne_train_flat_PULocationID = [i[:sizeof_train_data] for i in tsne_PULocationID]\n",
    "tsne_train_flat_weekday = [i[:sizeof_train_data] for i in tsne_weekday]\n",
    "tsne_train_flat_output = [i[:sizeof_train_data] for i in output]\n",
    "tsne_train_flat_exp_avg = [i[:sizeof_train_data] for i in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the rest of the timestamp values i.e 30% of sizeof_train_data + sizeof_test_data (total timestamps) for our test data\n",
    "\n",
    "tsne_test_flat_PULocationID = [i[sizeof_train_data:] for i in tsne_PULocationID]\n",
    "tsne_test_flat_weekday = [i[sizeof_train_data:] for i in tsne_weekday]\n",
    "tsne_test_flat_output = [i[sizeof_train_data:] for i in output]\n",
    "tsne_test_flat_exp_avg = [i[sizeof_train_data:] for i in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above contains values in the form of list of lists (i.e. list of values of each region), here we make all of them in one list\n",
    "train_new_features = []\n",
    "for i in range(0,nyc_regions_cnt):\n",
    "    train_new_features.extend(train_features[i])\n",
    "test_new_features = []\n",
    "for i in range(0,nyc_regions_cnt):\n",
    "    test_new_features.extend(test_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_train_PULocationID = sum(tsne_train_flat_PULocationID, [])\n",
    "tsne_train_weekday = sum(tsne_train_flat_weekday, [])\n",
    "tsne_train_output = sum(tsne_train_flat_output, [])\n",
    "tsne_train_exp_avg = sum(tsne_train_flat_exp_avg,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_test_PULocationID = sum(tsne_test_flat_PULocationID, [])\n",
    "tsne_test_weekday = sum(tsne_test_flat_weekday, [])\n",
    "tsne_test_output = sum(tsne_test_flat_output, [])\n",
    "tsne_test_exp_avg = sum(tsne_test_flat_exp_avg,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2538372, 8)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data frame for our train data\n",
    "columns = ['ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "df_train = pd.DataFrame(data=train_new_features, columns=columns) \n",
    "# df_train['lat'] = tsne_train_lat\n",
    "# df_train['lon'] = tsne_train_lon\n",
    "\n",
    "df_train['PULocationID'] = tsne_train_PULocationID\n",
    "df_train['weekday'] = tsne_train_weekday\n",
    "df_train['exp_avg'] = tsne_train_exp_avg\n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1087923, 8)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data frame for our train data\n",
    "df_test = pd.DataFrame(data=test_new_features, columns=columns) \n",
    "# df_test['lat'] = tsne_test_lat\n",
    "# df_test['lon'] = tsne_test_lon\n",
    "\n",
    "df_test['PULocationID'] = tsne_test_PULocationID\n",
    "df_test['weekday'] = tsne_test_weekday\n",
    "df_test['exp_avg'] = tsne_test_exp_avg\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>trip_times</th>\n",
       "      <th>pickup_times</th>\n",
       "      <th>Speed</th>\n",
       "      <th>pickup_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4</td>\n",
       "      <td>114</td>\n",
       "      <td>10.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.950000</td>\n",
       "      <td>1.561909e+09</td>\n",
       "      <td>7.547170</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.97</td>\n",
       "      <td>4</td>\n",
       "      <td>239</td>\n",
       "      <td>29.16</td>\n",
       "      <td>4.86</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>1.561909e+09</td>\n",
       "      <td>17.388350</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4</td>\n",
       "      <td>230</td>\n",
       "      <td>20.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.083333</td>\n",
       "      <td>1.561908e+09</td>\n",
       "      <td>10.867925</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "      <td>13.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>1.561909e+09</td>\n",
       "      <td>25.200000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4</td>\n",
       "      <td>255</td>\n",
       "      <td>22.45</td>\n",
       "      <td>5.15</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>1.561910e+09</td>\n",
       "      <td>13.907285</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5678283</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.67</td>\n",
       "      <td>263</td>\n",
       "      <td>239</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.766667</td>\n",
       "      <td>1.564543e+09</td>\n",
       "      <td>10.160677</td>\n",
       "      <td>4393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5678284</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.51</td>\n",
       "      <td>263</td>\n",
       "      <td>151</td>\n",
       "      <td>33.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.564544e+09</td>\n",
       "      <td>10.040000</td>\n",
       "      <td>4395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5678285</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.52</td>\n",
       "      <td>263</td>\n",
       "      <td>168</td>\n",
       "      <td>27.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.766667</td>\n",
       "      <td>1.564550e+09</td>\n",
       "      <td>11.253996</td>\n",
       "      <td>4406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5678286</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.52</td>\n",
       "      <td>263</td>\n",
       "      <td>74</td>\n",
       "      <td>26.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.316667</td>\n",
       "      <td>1.564550e+09</td>\n",
       "      <td>9.463779</td>\n",
       "      <td>4405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5678287</th>\n",
       "      <td>NaN</td>\n",
       "      <td>18.54</td>\n",
       "      <td>263</td>\n",
       "      <td>91</td>\n",
       "      <td>68.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.564559e+09</td>\n",
       "      <td>13.905000</td>\n",
       "      <td>4420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5589147 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       "0                    1.0           1.00             4           114   \n",
       "1                    2.0           5.97             4           239   \n",
       "2                    1.0           4.00             4           230   \n",
       "3                    1.0           2.80             4            88   \n",
       "4                    1.0           3.50             4           255   \n",
       "...                  ...            ...           ...           ...   \n",
       "5678283              NaN           2.67           263           239   \n",
       "5678284              NaN           2.51           263           151   \n",
       "5678285              NaN           3.52           263           168   \n",
       "5678286              NaN           3.52           263            74   \n",
       "5678287              NaN          18.54           263            91   \n",
       "\n",
       "         total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       "0               10.30        0.00    7.950000  1.561909e+09   7.547170   \n",
       "1               29.16        4.86   20.600000  1.561909e+09  17.388350   \n",
       "2               20.80        0.00   22.083333  1.561908e+09  10.867925   \n",
       "3               13.80        0.00    6.666667  1.561909e+09  25.200000   \n",
       "4               22.45        5.15   15.100000  1.561910e+09  13.907285   \n",
       "...               ...         ...         ...           ...        ...   \n",
       "5678283         22.00        0.00   15.766667  1.564543e+09  10.160677   \n",
       "5678284         33.51        0.00   15.000000  1.564544e+09  10.040000   \n",
       "5678285         27.37        0.00   18.766667  1.564550e+09  11.253996   \n",
       "5678286         26.41        0.00   22.316667  1.564550e+09   9.463779   \n",
       "5678287         68.10        0.00   80.000000  1.564559e+09  13.905000   \n",
       "\n",
       "         pickup_bins  \n",
       "0                  4  \n",
       "1                  2  \n",
       "2                  1  \n",
       "3                  3  \n",
       "4                  4  \n",
       "...              ...  \n",
       "5678283         4393  \n",
       "5678284         4395  \n",
       "5678285         4406  \n",
       "5678286         4405  \n",
       "5678287         4420  \n",
       "\n",
       "[5589147 rows x 10 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "months_frame[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4464, 4032, 4464, 4320, 4464, 4320, 4464, 4464, 4320, 4464, 4320, 4464]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickup_bins_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(months_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date-time: 2019-07-09 22:08:00\n",
      "1562677680.0\n",
      "27348\n",
      "target_month =  7\n",
      "target_bin =  1284\n",
      "2\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "input_string = '2019-07-09 22:08:00'\n",
    "input_time = datetime.datetime.strptime(input_string, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print('Date-time:', input_time)\n",
    "\n",
    "\n",
    "def convert_to_unix(s):\n",
    "    return time.mktime(s.timetuple())\n",
    "\n",
    "start_pickup_unix = 1546268400 #local time 201901010000\n",
    "\n",
    "def calculate_bin(time):\n",
    "    return (int((time-start_pickup_unix)/600)) # !!!!!!!!!!!! 말해보자 -> 33(330분)을 더해주는 이유? [안 더하면 요일계산 잘됨]\n",
    "\n",
    "unix_time = convert_to_unix(input_time)\n",
    "print(unix_time)\n",
    "\n",
    "bin = calculate_bin(unix_time)\n",
    "print(bin)\n",
    "\n",
    "def calculate_month(bin):\n",
    "    month = 0\n",
    "    check = 0\n",
    "    while bin>0:\n",
    "        bin -= pickup_bins_len[check]\n",
    "        check+=1\n",
    "        month+=1\n",
    "    return month\n",
    "\n",
    "## 입력받은 날짜의 월 계산\n",
    "target_month = calculate_month(bin)\n",
    "print(\"target_month = \", target_month)\n",
    "\n",
    "## 입력받은 날짜의 월에 해당하는 bin 구하기\n",
    "target_bin = bin\n",
    "for i in range(target_month-1):\n",
    "    target_bin -= pickup_bins_len[i]\n",
    "\n",
    "print(\"target_bin = \", target_bin)\n",
    "\n",
    "weekday = ((int(bin/144))%7+day_of_the_week_dict[base_year+1])%7\n",
    "print(weekday)\n",
    "\n",
    "exp_avg_test = predict_list[3][bin] # predict_list[n][bin] -> n-1번 지역의 bin에서의 exp_avg\n",
    "print(exp_avg_test)\n",
    "\n",
    "before_frame = months_frame[target_month - 1][['PULocationID','pickup_bins']] #target_frame = 예측년도를 전처리 한 프레임 (jan_base_year_frame)\n",
    "after_frame = before_frame.loc[(before_frame['pickup_bins'] <target_bin) & (before_frame['pickup_bins']>=target_bin-5)]\n",
    "\n",
    "# for i in range(1, 2): ## 1~5번째 전 bin의 탑승 수를 맨해튼 모든 지역별로 돌려서 저장 \n",
    "# ft_1이 10분 전\n",
    "# 말해볼 것: 달별로 하나씩 뽑아서 평균값으로?\n",
    "\n",
    "input_list = []\n",
    "\n",
    "columns = ['ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "temp = 0 \n",
    "for i in nyc_region_number.values:\n",
    "    temp_list = [0 for i in range(5)]\n",
    "    temp1_frame = after_frame.loc[(after_frame['PULocationID']==i)]\n",
    "    globals()['region_{}_frame'.format(i)] = pd.DataFrame(columns = columns)\n",
    "    for j in range(1,6):\n",
    "        temp2_frame = temp1_frame.loc[(temp1_frame['pickup_bins'] == target_bin-j)] # ft_1, ft_2, ft_3, ft_4, ft_5 구하기\n",
    "        temp_list[5-j] = len(temp2_frame)\n",
    "        # globals()['region_{}_bins_{}'.format(i, j)] = len(temp2_frame)\n",
    "    eval('region_' + str(i) + '_frame').loc[str(i)+ '번지역'] = temp_list\n",
    "    eval('region_' + str(i) + '_frame')['weekday'] = weekday\n",
    "    eval('region_' + str(i) + '_frame')['exp_avg'] = predict_list[temp][bin]\n",
    "    eval('region_' + str(i) + '_frame')['PULocationID'] = i\n",
    "    input_list.append(eval('region_' + str(i) + '_frame'))\n",
    "    temp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft_5</th>\n",
       "      <th>ft_4</th>\n",
       "      <th>ft_3</th>\n",
       "      <th>ft_2</th>\n",
       "      <th>ft_1</th>\n",
       "      <th>weekday</th>\n",
       "      <th>exp_avg</th>\n",
       "      <th>PULocationID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4번지역</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ft_5  ft_4  ft_3  ft_2  ft_1  weekday  exp_avg  PULocationID\n",
       "4번지역     2     1     3     4     4        2        2             4"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list[3].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft_5</th>\n",
       "      <th>ft_4</th>\n",
       "      <th>ft_3</th>\n",
       "      <th>ft_2</th>\n",
       "      <th>ft_1</th>\n",
       "      <th>weekday</th>\n",
       "      <th>exp_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "      <td>45</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ft_5  ft_4  ft_3  ft_2  ft_1  weekday  exp_avg\n",
       "0    43    43    42    45    33        2       44"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_100_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find more about LinearRegression function here http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
    "\n",
    "# some of methods of LinearRegression()\n",
    "# fit(X, y[, sample_weight])\tFit linear model.\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict using the linear model\n",
    "# score(X, y[, sample_weight])\tReturns the coefficient of determination R^2 of the prediction.\n",
    "# set_params(**params)\tSet the parameters of this estimator.\n",
    "# -----------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1-2-copy-8/\n",
    "# -----------------------\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_reg=LinearRegression().fit(df_train, tsne_train_output)\n",
    "\n",
    "y_pred = lr_reg.predict(df_test)\n",
    "lr_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = lr_reg.predict(df_train)\n",
    "lr_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_features=&#x27;sqrt&#x27;, min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_features=&#x27;sqrt&#x27;, min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_features='sqrt', min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned random forest regressor on our train data\n",
    "# find more about LinearRegression function here http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion=’mse’, max_depth=None, min_samples_split=2, \n",
    "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# apply(X)\tApply trees in the forest to X, return leaf indices.\n",
    "# decision_path(X)\tReturn the decision path in the forest\n",
    "# fit(X, y[, sample_weight])\tBuild a forest of trees from the training set (X, y).\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict regression target for X.\n",
    "# score(X, y[, sample_weight])\tReturns the coefficient of determination R^2 of the prediction.\n",
    "# -----------------------\n",
    "# video link1: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/regression-using-decision-trees-2/\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "regr1 = RandomForestRegressor(max_features='sqrt',min_samples_leaf=4,min_samples_split=3,n_estimators=40, n_jobs=-1)\n",
    "regr1.fit(df_train, tsne_train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data using our trained random forest model \n",
    "\n",
    "# the models regr1 is already hyper parameter tuned\n",
    "# the parameters that we got above are found using grid search\n",
    "\n",
    "y_pred = regr1.predict(df_test)\n",
    "rndf_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = regr1.predict(df_train)\n",
    "rndf_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.57678755])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict = regr1.predict(input_list[3])\n",
    "test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4번지역    4\n",
       "Name: PULocationID, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list[0]['PULocationID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    OBJECTID  predict_number\n",
      "0        4.0        3.072544\n",
      "1       12.0        4.886163\n",
      "2       13.0       18.021081\n",
      "3       24.0       12.576788\n",
      "4       41.0       23.861578\n",
      "..       ...             ...\n",
      "64     246.0       73.614238\n",
      "65     249.0       75.146123\n",
      "66     261.0       41.332588\n",
      "67     262.0       38.001822\n",
      "68     263.0       56.370002\n",
      "\n",
      "[69 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# arr = np.array([])\n",
    "# arr = np.append(arr, np.array([1, 2, 3]))\n",
    "# arr = np.append(arr, np.array([4, 5]))\n",
    "# print(arr)\n",
    "\n",
    "jinsu_column = ['OBJECTID', 'predict_number']\n",
    "predict_df = pd.DataFrame(columns = jinsu_column)\n",
    "jinsu_index = 0\n",
    "for i in input_list:\n",
    "    temp_result = np.array([i['PULocationID'].iloc[0]])\n",
    "    temp_predict_value = regr1.predict(i)\n",
    "    temp_result = np.append(temp_result, temp_predict_value)\n",
    "    predict_df.loc[jinsu_index] = temp_result\n",
    "    jinsu_index += 1\n",
    "\n",
    "print(predict_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft_5</th>\n",
       "      <th>ft_4</th>\n",
       "      <th>ft_3</th>\n",
       "      <th>ft_2</th>\n",
       "      <th>ft_1</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>weekday</th>\n",
       "      <th>exp_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087918</th>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087919</th>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087920</th>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087921</th>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087922</th>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1087923 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ft_5  ft_4  ft_3  ft_2  ft_1  PULocationID  weekday  exp_avg\n",
       "0           1     1     1     2     2             1        5        1\n",
       "1           1     1     2     2     2             1        5        1\n",
       "2           1     2     2     2     2             1        5        1\n",
       "3           2     2     2     2     4             1        5        3\n",
       "4           2     2     2     4     4             1        5        3\n",
       "...       ...   ...   ...   ...   ...           ...      ...      ...\n",
       "1087918    37    40    32    53    39            69        2       41\n",
       "1087919    40    32    53    39    28            69        2       31\n",
       "1087920    32    53    39    28    33            69        2       32\n",
       "1087921    53    39    28    33    30            69        2       30\n",
       "1087922    39    28    33    30    35            69        2       33\n",
       "\n",
       "[1087923 rows x 8 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.35347556,  9.57000534, 10.36064495, ...,  3.9087823 ,\n",
       "        2.48008677,  5.08856498])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ft_5', 'ft_4', 'ft_3', 'ft_2', 'ft_1', 'PULocationID', 'weekday',\n",
      "       'exp_avg'],\n",
      "      dtype='object')\n",
      "[0.07684949 0.07512184 0.18006786 0.13625224 0.23265497 0.01096237\n",
      " 0.00264471 0.28544652]\n"
     ]
    }
   ],
   "source": [
    "#feature importances based on analysis using random forest\n",
    "print (df_train.columns)\n",
    "print (regr1.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft_5</th>\n",
       "      <th>ft_4</th>\n",
       "      <th>ft_3</th>\n",
       "      <th>ft_2</th>\n",
       "      <th>ft_1</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>weekday</th>\n",
       "      <th>exp_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087918</th>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087919</th>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087920</th>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087921</th>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087922</th>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1087923 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ft_5  ft_4  ft_3  ft_2  ft_1  PULocationID  weekday  exp_avg\n",
       "0           1     1     1     2     2             1        5        1\n",
       "1           1     1     2     2     2             1        5        1\n",
       "2           1     2     2     2     2             1        5        1\n",
       "3           2     2     2     2     4             1        5        3\n",
       "4           2     2     2     4     4             1        5        3\n",
       "...       ...   ...   ...   ...   ...           ...      ...      ...\n",
       "1087918    37    40    32    53    39            69        2       41\n",
       "1087919    40    32    53    39    28            69        2       31\n",
       "1087920    32    53    39    28    33            69        2       32\n",
       "1087921    53    39    28    33    30            69        2       30\n",
       "1087922    39    28    33    30    35            69        2       33\n",
       "\n",
       "[1087923 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 14,\n",
       " 16,\n",
       " 16,\n",
       " 18,\n",
       " 19,\n",
       " 17,\n",
       " 14,\n",
       " 15,\n",
       " 13,\n",
       " 11,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 9,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 13,\n",
       " 12,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rndf_train_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using XgBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "             random_state=0, reg_alpha=200, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "             random_state=0, reg_alpha=200, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor='auto',\n",
       "             random_state=0, reg_alpha=200, ...)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned Xg-Boost regressor on our train data\n",
    "\n",
    "# find more about XGBRegressor function here http://xgboost.readthedocs.io/en/latest/python/python_api.html?#module-xgboost.sklearn\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# xgboost.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', \n",
    "# booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, \n",
    "# colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, \n",
    "# missing=None, **kwargs)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(data, output_margin=False, ntree_limit=0) : Predict with data. NOTE: This function is not thread safe.\n",
    "# get_score(importance_type='weight') -> get the feature importance\n",
    "# -----------------------\n",
    "# video link1: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/regression-using-decision-trees-2/\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "x_model = xgb.XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=3,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " reg_alpha=200, reg_lambda=200,\n",
    " colsample_bytree=0.8,nthread=4)\n",
    "x_model.fit(df_train, tsne_train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting with our trained Xg-Boost regressor\n",
    "# the models x_model is already hyper parameter tuned\n",
    "# the parameters that we got above are found using grid search\n",
    "\n",
    "y_pred = x_model.predict(df_test)\n",
    "xgb_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = x_model.predict(df_train)\n",
    "xgb_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ft_5': 945.0,\n",
       " 'ft_4': 745.0,\n",
       " 'ft_3': 813.0,\n",
       " 'ft_2': 970.0,\n",
       " 'ft_1': 1122.0,\n",
       " 'PULocationID': 1153.0,\n",
       " 'weekday': 189.0,\n",
       " 'exp_avg': 994.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature importances\n",
    "x_model.get_booster().get_score(importance_type=\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the error metric values for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_R2=[]\n",
    "test_R2=[]\n",
    "\n",
    "train_R2.append(r2_score(tsne_train_output,df_train['ft_1'].values))\n",
    "train_R2.append(r2_score(tsne_train_output,df_train['exp_avg'].values))\n",
    "train_R2.append(r2_score(tsne_train_output,rndf_train_predictions))\n",
    "train_R2.append(r2_score(tsne_train_output,xgb_train_predictions))\n",
    "train_R2.append(r2_score(tsne_train_output,lr_train_predictions))\n",
    "\n",
    "test_R2.append(r2_score(tsne_test_output,df_test['ft_1'].values))\n",
    "test_R2.append(r2_score(tsne_test_output,df_test['exp_avg'].values))\n",
    "test_R2.append(r2_score(tsne_test_output,rndf_test_predictions))\n",
    "test_R2.append(r2_score(tsne_test_output,xgb_test_predictions))\n",
    "test_R2.append(r2_score(tsne_test_output,lr_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 1 ~ 12 Manhattan R2\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Baseline Model\t\t\t\tTrain: \t0.9261341021248126\tTest: \t0.9279697110296186\n",
      "Exponential Averages Forecasting\tTrain: \t0.9353202829543953\tTest: \t0.93690114529243\n",
      "Random Forest Regression\t\tTrain: \t0.9642250369647256\tTest: \t0.9404143153398121\n",
      "XgBoost Regression\t\t\tTrain: \t0.9399804186815103\tTest: \t0.9409247248608636\n",
      "Linear Regression\t\t\tTrain: \t0.937148019499591\tTest: \t0.9386946289815727\n"
     ]
    }
   ],
   "source": [
    "print(f\"{base_year+1} 1 ~ {base_month_count} {region} R2\")\n",
    "print (\"--------------------------------------------------------------------------------------------------------\")\n",
    "print (\"Baseline Model\\t\\t\\t\",\"Train: \",train_R2[0],\"Test: \",test_R2[0],sep='\\t')\n",
    "print (\"Exponential Averages Forecasting\",\"Train: \",train_R2[1],\"Test: \",test_R2[1],sep='\\t')\n",
    "print (\"Random Forest Regression\\t\",\"Train: \",train_R2[2],\"Test: \",test_R2[2],sep='\\t')\n",
    "print (\"XgBoost Regression\\t\\t\",\"Train: \",train_R2[3],\"Test: \",test_R2[3],sep='\\t')\n",
    "print (\"Linear Regression\\t\\t\",\"Train: \",train_R2[4],\"Test: \",test_R2[4],sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MAE=[]\n",
    "test_MAE=[]\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,df_train['ft_1'].values))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,df_train['exp_avg'].values))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,rndf_train_predictions))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,xgb_train_predictions))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,lr_train_predictions))\n",
    "\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,df_test['ft_1'].values))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,df_test['exp_avg'].values))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,rndf_test_predictions))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,xgb_test_predictions))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,lr_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 1 ~ 12 Manhattan MAE\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Baseline Model\t\t\t\tTrain: \t4.155000921850698\tTest: \t4.0590041758470035\n",
      "Exponential Averages Forecasting\tTrain: \t3.8772323363163474\tTest: \t3.787620998912607\n",
      "Random Forest Regression\t\tTrain: \t2.9204738312587755\tTest: \t3.7134668538122644\n",
      "XgBoost Regression\t\t\tTrain: \t3.7752823463227614\tTest: \t3.6963663788705636\n",
      "Linear Regression\t\t\tTrain: \t3.8324524537774605\tTest: \t3.743439563277916\n"
     ]
    }
   ],
   "source": [
    "print(f\"{base_year+1} 1 ~ {base_month_count} {region} MAE\")\n",
    "print (\"--------------------------------------------------------------------------------------------------------\")\n",
    "print (\"Baseline Model\\t\\t\\t\",\"Train: \",train_MAE[0],\"Test: \",test_MAE[0],sep='\\t')\n",
    "print (\"Exponential Averages Forecasting\",\"Train: \",train_MAE[1],\"Test: \",test_MAE[1],sep='\\t')\n",
    "print (\"Random Forest Regression\\t\",\"Train: \",train_MAE[2],\"Test: \",test_MAE[2],sep='\\t')\n",
    "print (\"XgBoost Regression\\t\\t\",\"Train: \",train_MAE[3],\"Test: \",test_MAE[3],sep='\\t')\n",
    "print (\"Linear Regression\\t\\t\",\"Train: \",train_MAE[4],\"Test: \",test_MAE[4],sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
