{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unix time: https://www.unixtimestamp.com/\n",
    "import datetime  # Convert to unix time\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time  # Convert to unix time\n",
    "import warnings\n",
    "\n",
    "import dask.dataframe as dd  # similar to pandas\n",
    "import matplotlib.pylab as plt\n",
    "# if numpy is not installed already : pip3 install numpy\n",
    "import numpy as np  # Do aritmetic operations on arrays\n",
    "import pandas as pd  # pandas to create small dataframes\n",
    "import seaborn as sns  # Plots\n",
    "# to install xgboost: pip3 install xgboost\n",
    "# if it didnt happen check install_xgboost.JPG\n",
    "import xgboost as xgb\n",
    "from matplotlib import rcParams  # Size of plots\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans  # Clustering\n",
    "# to install sklearn: pip install -U scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (mean_absolute_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_squared_error, r2_score)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_year = 2018\n",
    "base_month_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    months_frame = []\n",
    "    months_groupby = []\n",
    "    for i in range(1,base_month_count+1):\n",
    "        tmp_frame = pd.read_parquet(f'preprocessing_yellow_tripdata_{base_year+1}_{i}.parquet',engine='pyarrow')\n",
    "        \n",
    "        tmp_groupby = tmp_frame[['PULocationID','pickup_bins','trip_distance']].groupby(['PULocationID','pickup_bins'], dropna=False).count()\n",
    "        tmp_tip_amount = tmp_frame[['PULocationID','pickup_bins','tip_amount']].groupby(['PULocationID','pickup_bins'], dropna=False).mean()\n",
    "        #print(tmp_groupby)\n",
    "        # print(\"-------------------------\")\n",
    "        #print(tmp_tip_amount)\n",
    "        # print(\"-------------------------\")\n",
    "        #tmp_groupby = pd.concat([tmp_trip_distance, tmp_tip_amount])\n",
    "        #print(tmp_groupby)\n",
    "        tmp_groupby = pd.merge(tmp_groupby, tmp_tip_amount, on = ['PULocationID','pickup_bins'], how = \"left\")\n",
    "\n",
    "        \n",
    "        print(tmp_groupby)\n",
    "        months_frame.append(tmp_frame)\n",
    "        months_groupby.append(tmp_groupby)\n",
    "    return months_frame, months_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          trip_distance  tip_amount\n",
      "PULocationID pickup_bins                           \n",
      "4            -52                      1    0.000000\n",
      "              0                       1    0.000000\n",
      "              1                       4    1.450000\n",
      "              2                      11    2.130909\n",
      "              3                       9    1.034444\n",
      "...                                 ...         ...\n",
      "263           4459                   17    1.778824\n",
      "              4460                   25    0.873200\n",
      "              4461                   23    1.158261\n",
      "              4462                   17    1.254706\n",
      "              4463                   24    1.257917\n",
      "\n",
      "[249105 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "months_frame, months_groupby = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴욕 지역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_df = pd.read_csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"Manhattan\"\n",
    "nyc_region = taxi_zone_df[taxi_zone_df['Borough'] == region]\n",
    "nyc_region_number = nyc_region['LocationID']\n",
    "nyc_regions_cnt = len(nyc_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464\n"
     ]
    }
   ],
   "source": [
    "# number of 10min indices for jan 2019= 24*31*60/10 = max_pickup_bins_len\n",
    "interval = 10\n",
    "days = [31,29,31,30,31,30,31,31,30,31,30,31]\n",
    "pickup_bins_len = []\n",
    "\n",
    "for day in days:\n",
    "    pickup_bins_len.append(int(24*60*day/interval))\n",
    "max_pickup_bins_len = max(pickup_bins_len)\n",
    "print(max_pickup_bins_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills a value of zero for every bin where no pickup data is present \n",
    "# the count_values: number pickps that are happened in each region for each 10min intravel\n",
    "# there wont be any value if there are no picksups.\n",
    "# values: number of unique bins\n",
    "\n",
    "# for every 10min intravel(pickup_bin) we will check it is there in our unique bin,\n",
    "# if it is there we will add the count_values[index] to smoothed data\n",
    "# if not we add smoothed data (which is calculated based on the methods that are discussed in the above markdown cell)\n",
    "# we finally return smoothed data\n",
    "def smoothing(count_values,values):\n",
    "    smoothed_regions=[] # stores list of final smoothed values of each reigion\n",
    "    ind=0\n",
    "    repeat=0 \n",
    "    smoothed_value=0\n",
    "    for r in range(1,nyc_regions_cnt+1):\n",
    "        smoothed_bins=[] #stores the final smoothed values\n",
    "        repeat=0\n",
    "        for i in range(max_pickup_bins_len):\n",
    "            if repeat!=0: # prevents iteration for a value which is already visited/resolved\n",
    "                repeat-=1\n",
    "                continue\n",
    "            if i in values[r-1]: #checks if the pickup-bin exists \n",
    "                smoothed_bins.append(count_values[ind-1]) # appends the value of the pickup bin if it exists\n",
    "            else:\n",
    "                if i!=0:\n",
    "                    right_hand_limit=0\n",
    "                    for j in range(i,max_pickup_bins_len):\n",
    "                        if  j not in values[r-1]: #searches for the left-limit or the pickup-bin value which has a pickup value\n",
    "                            continue\n",
    "                        else:\n",
    "                            right_hand_limit=j\n",
    "                            break\n",
    "                    if right_hand_limit==0:\n",
    "                    #Case 1: When we have the last/last few values are found to be missing,hence we have no right-limit here\n",
    "                        smoothed_value=count_values[ind-1]*1.0/((max_pickup_bins_len-1-i)+2)*1.0                               \n",
    "                        for j in range(i,max_pickup_bins_len):                              \n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n",
    "                        repeat=(max_pickup_bins_len-1-i)\n",
    "                        ind-=1\n",
    "                    else:\n",
    "                    #Case 2: When we have the missing values between two known values\n",
    "                        smoothed_value=(count_values[ind-1]+count_values[ind])*1.0/((right_hand_limit-i)+2)*1.0             \n",
    "                        for j in range(i,right_hand_limit+1):\n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n",
    "                        repeat=(right_hand_limit-i)\n",
    "                else:\n",
    "                    #Case 3: When we have the first/first few values are found to be missing,hence we have no left-limit here\n",
    "                    right_hand_limit=0\n",
    "                    for j in range(i,max_pickup_bins_len):\n",
    "                        if  j not in values[r-1]:\n",
    "                            continue\n",
    "                        else:\n",
    "                            right_hand_limit=j\n",
    "                            break\n",
    "                    smoothed_value=count_values[ind]*1.0/((right_hand_limit-i)+1)*1.0\n",
    "                    for j in range(i,right_hand_limit+1):\n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                    repeat=(right_hand_limit-i)\n",
    "            ind+=1\n",
    "        smoothed_regions.extend(smoothed_bins)\n",
    "    return smoothed_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unq_pickup_bins(frame):\n",
    "    values = []\n",
    "    for i in nyc_region_number.values:\n",
    "    # for i in range(1,266):\n",
    "        new = frame[frame['PULocationID'] == i]\n",
    "        list_unq = list(set(new['pickup_bins']))\n",
    "        list_unq.sort()\n",
    "        values.append(list_unq)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_unique = []\n",
    "for frame in months_frame:\n",
    "    months_unique.append(return_unq_pickup_bins(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "4464\n",
      "69\n",
      "4464\n"
     ]
    }
   ],
   "source": [
    "months_smooth = []\n",
    "months_smooth_tip = []\n",
    "for groupby, unique in zip(months_groupby, months_unique):\n",
    "    # smoothing을 할 것인가 filling을 할것인가\n",
    "    months_smooth.append(smoothing(groupby['trip_distance'].values,unique))\n",
    "    months_smooth_tip.append(smoothing(groupby['tip_amount'].values,unique))\n",
    "    \n",
    "# Making list of all the values of pickup data in every bin for a period of 3 months and storing them region-wise \n",
    "regions_cum = []\n",
    "regions_cum_tip = []\n",
    "\n",
    "# number of 10min indices for jan 2019= 24*31*60/10 = 4464      # pickup_bins_len[0]\n",
    "# number of 10min indices for jan 2020 = 24*31*60/10 = 4464     # pickup_bins_len[0]\n",
    "# number of 10min indices for feb 2020 = 24*29*60/10 = 4176     # pickup_bins_len[1]\n",
    "# number of 10min indices for march 2020 = 24*31*60/10 = 4464   # pickup_bins_len[2]\n",
    "# regions_cum: it will contain 40 lists, each list will contain 4464+4176+4464 values which represents the number of pickups \n",
    "# that are happened for three months in 2016 data\n",
    "\n",
    "# nyc_regions_cnt개의 맨허튼 지역\n",
    "for i in range(1,nyc_regions_cnt+1):\n",
    "    cum = []\n",
    "    cum_tip = []\n",
    "    for index, smooth in enumerate(months_smooth):\n",
    "        cum += smooth[pickup_bins_len[index]*(i-1):pickup_bins_len[index]*i]\n",
    "    for index, smooth in enumerate(months_smooth_tip):\n",
    "        cum_tip += smooth[pickup_bins_len[index]*(i-1):pickup_bins_len[index]*i]\n",
    "    \n",
    "    regions_cum.append(cum)\n",
    "    regions_cum_tip.append(cum_tip)\n",
    "\n",
    "print(len(regions_cum))\n",
    "print(len(regions_cum[0]))\n",
    "print(len(regions_cum_tip))\n",
    "print(len(regions_cum_tip[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing data to be split into train and test, The below prepares data in cumulative form which will be later split into test and train\n",
    "# number of 10min indices for jan 2019= 24*31*60/10 = 4464      # pickup_bins_len[0]\n",
    "# number of 10min indices for jan 2020 = 24*31*60/10 = 4464     # pickup_bins_len[0]\n",
    "# number of 10min indices for feb 2020 = 24*29*60/10 = 4176     # pickup_bins_len[1]\n",
    "# number of 10min indices for march 2020 = 24*31*60/10 = 4464   # pickup_bins_len[2]\n",
    "# regions_cum: it will contain 40 lists, each list will contain 4464+4176+4464 values which represents the number of pickups \n",
    "# that are happened for three months in 2016 data\n",
    "\n",
    "# print(len(regions_cum))\n",
    "# 265\n",
    "# print(len(regions_cum[0]))\n",
    "# 4368\n",
    "\n",
    "\n",
    "# we take number of pickups that are happened in last 5 intravels\n",
    "number_of_time_stamps = 5\n",
    "\n",
    "# output varaible\n",
    "# it is list of lists\n",
    "# it will contain number of pickups 4368 for each cluster\n",
    "# len(regions_cum[0]) == 4368\n",
    "output = []\n",
    "output_tip = []\n",
    "sum(pickup_bins_len[:base_month_count])\n",
    "# 우리 데이터\n",
    "# len(regions_cum[0]) - 5(:= # of colunms)\n",
    "# 4368 - 5 = 4363\n",
    "# 13104 - 5 = 13099\n",
    "\n",
    "# tsne_lat will contain 13104-5=13099 times lattitude of cluster center for every cluster\n",
    "# Ex: [[cent_lat 13099times],[cent_lat 13099times], [cent_lat 13099times].... 40 lists]\n",
    "# it is list of lists\n",
    "# tsne_lat = []\n",
    "\n",
    "# tsne_lon will contain 13104-5=13099 times logitude of cluster center for every cluster\n",
    "# Ex: [[cent_long 13099times],[cent_long 13099times], [cent_long 13099times].... 40 lists]\n",
    "# it is list of lists\n",
    "# tsne_lon = []\n",
    "\n",
    "# 우리는 lat, lon 대신에 목적지 ID (PULocationID: 출발지, DOLocationID: 도착지)를 사용할 것이다.\n",
    "tsne_PULocationID = []\n",
    "\n",
    "#tsne_Tip_amount = []\n",
    "# we will code each day \n",
    "# sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5, sat=6\n",
    "# for every cluster we will be adding 13099 values, each value represent to which day of the week that pickup bin belongs to\n",
    "# it is list of lists\n",
    "tsne_weekday = []\n",
    "\n",
    "# its an numbpy array, of shape (523960, 5)\n",
    "# each row corresponds to an entry in out data\n",
    "# for the first row we will have [f0,f1,f2,f3,f4] fi=number of pickups happened in i+1th 10min intravel(bin)\n",
    "# the second row will have [f1,f2,f3,f4,f5]\n",
    "# the third row will have [f2,f3,f4,f5,f6]\n",
    "# and so on...\n",
    "tsne_feature = []\n",
    "\n",
    "\n",
    "tsne_feature = [0]*number_of_time_stamps\n",
    "for i in range(1,nyc_regions_cnt+1):\n",
    "    # tsne_lat.append([kmeans.cluster_centers_[i][0]]*13099) # kmeans.cluster_centers_[i][0] := Coordinates of cluster centers. 클러스트 센터의 상관계수\n",
    "    # tsne_lon.append([kmeans.cluster_centers_[i][1]]*13099)\n",
    "\n",
    "    # tsne_PULocationID\n",
    "    tsne_PULocationID.append([i]*(len(regions_cum[0]) - 5))\n",
    "    \n",
    "    #tsne_Tip_amount.append([i]*(len(regions_cum[0]) - 5))\n",
    "\n",
    "    day_of_the_week_dict = {2015: 4, 2016: 5, 2017: 1, 2018:1, 2019:2, 2020:3, 2021:5, 2022:6}\n",
    "    # jan 1st 2016 is thursday, so we start our day from 4: \"(int(k/144))%7+4\"\n",
    "    # our prediction start from 5th 10min intravel since we need to have number of pickups that are happened in last 5 pickup bins\n",
    "    \n",
    "    # jan 1st 2020 is tue -> 3\n",
    "    tsne_weekday.append([int(((int(k/144))%7+day_of_the_week_dict[base_year+1])%7) for k in range(5,sum(pickup_bins_len[:base_month_count]))])\n",
    "\n",
    "    # jan 1st 2021 is fri -> 5\n",
    "    # tsne_weekday.append([int(((int(k/144))%7+5)%7) for k in range(5,sum(pickup_bins_len[:3]))])\n",
    "    # regions_cum is a list of lists [[x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], .. 40 lsits]\n",
    "    \n",
    "    # 우리 데이터 \n",
    "    # regions_cum [[x_1,x_2,...,x_{len(regions_cum[0]) - 5}],...265 lists] len(regions_cum[0]) - 5 = 4381\n",
    "    tsne_feature = np.vstack((tsne_feature, [regions_cum[i-1][r:r+number_of_time_stamps] for r in range(0,len(regions_cum[i-1])-number_of_time_stamps)]))\n",
    "\n",
    "    output.append(regions_cum[i-1][5:])\n",
    "    output_tip.append(regions_cum_tip[i-1][5:])\n",
    "tsne_feature = tsne_feature[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307671\n",
      "307671\n",
      "307671\n",
      "307671\n",
      "307671\n",
      "307671\n"
     ]
    }
   ],
   "source": [
    "print(tsne_feature.shape[0])\n",
    "print(len(tsne_weekday)*len(tsne_weekday[0]))\n",
    "print(len(output)*len(output[0]))\n",
    "print(len(output_tip)*len(output_tip[0]))\n",
    "print(nyc_regions_cnt*(len(regions_cum[0])-5))\n",
    "print(len(tsne_PULocationID)*len(tsne_PULocationID[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions of exponential moving averages to be used as a feature in cumulative form\n",
    "\n",
    "# upto now we computed 8 features for every data point that starts from 50th min of the day\n",
    "# 1. cluster center lattitude\n",
    "# 2. cluster center longitude\n",
    "# 3. day of the week \n",
    "# 4. f_t_1: number of pickups that are happened previous t-1th 10min intravel\n",
    "# 5. f_t_2: number of pickups that are happened previous t-2th 10min intravel\n",
    "# 6. f_t_3: number of pickups that are happened previous t-3th 10min intravel\n",
    "# 7. f_t_4: number of pickups that are happened previous t-4th 10min intravel\n",
    "# 8. f_t_5: number of pickups that are happened previous t-5th 10min intravel\n",
    "\n",
    "# from the baseline models we said the exponential weighted moving avarage gives us the best error\n",
    "# we will try to add the same exponential weighted moving avarage at t as a feature to our data\n",
    "# exponential weighted moving avarage => p'(t) = alpha*p'(t-1) + (1-alpha)*P(t-1) \n",
    "alpha=0.3\n",
    "\n",
    "# it is a temporary array that store exponential weighted moving avarage for each 10min intravel, \n",
    "# for each cluster it will get reset\n",
    "# for every cluster it contains 13104 values\n",
    "predicted_values=[]\n",
    "predicted_values_tip=[]\n",
    "\n",
    "# it is similar like tsne_lat\n",
    "# it is list of lists\n",
    "# predict_list is a list of lists [[x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], .. 40 lsits]\n",
    "predict_list = []\n",
    "predict_list_tip = []\n",
    "tsne_flat_exp_avg = []\n",
    "for r in range(1,nyc_regions_cnt+1):\n",
    "    for i in range(0,len(regions_cum[0])):\n",
    "        if i==0:\n",
    "            predicted_value= regions_cum[r-1][0]\n",
    "            predicted_values.append(0)\n",
    "            predicted_value_tip= regions_cum_tip[r-1][0]\n",
    "            predicted_values_tip.append(0)\n",
    "            continue\n",
    "        predicted_values.append(predicted_value)\n",
    "        predicted_value =int((alpha*predicted_value) + (1-alpha)*(regions_cum[r-1][i]))\n",
    "        predicted_values_tip.append(predicted_value_tip)\n",
    "        predicted_value_tip =(alpha*predicted_value_tip) + (1-alpha)*(regions_cum_tip[r-1][i])\n",
    "    \n",
    "    predict_list.append(predicted_values[5:])\n",
    "    predicted_values=[]\n",
    "    predict_list_tip.append(predicted_values_tip[5:])\n",
    "    predicted_values_tip=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data : 3121\n",
      "size of test data : 1337\n"
     ]
    }
   ],
   "source": [
    "# train, test split : 70% 30% split\n",
    "# Before we start predictions using the tree based regression models we take 3 months of 2016 pickup data \n",
    "# and split it such that for every region we have 70% data in train and 30% in test,\n",
    "# ordered date-wise for every region\n",
    "\n",
    "sizeof_train_data = int((len(regions_cum[0])-5)*0.7)\n",
    "sizeof_test_data = int((len(regions_cum[0])-5)*0.3)\n",
    "\n",
    "\n",
    "print(\"size of train data :\", sizeof_train_data)\n",
    "print(\"size of test data :\", sizeof_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting first 91nyc_regions_cnt timestamp values i.e 70% of 13099 (total timestamps) for our training data\n",
    "train_features =  [tsne_feature[i*(len(regions_cum[0])-5):((len(regions_cum[0])-5)*i+sizeof_train_data)] for i in range(0,nyc_regions_cnt)]\n",
    "\n",
    "test_features = [tsne_feature[((len(regions_cum[0])-5)*(i))+sizeof_train_data:(len(regions_cum[0])-5)*(i+1)] for i in range(0,nyc_regions_cnt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data clusters 69 Number of data points in trian data 3121 Each data point contains 5 features\n",
      "Number of data clusters 69 Number of data points in test data 1338 Each data point contains 5 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data clusters\",len(train_features), \"Number of data points in trian data\", len(train_features[0]), \"Each data point contains\", len(train_features[0][0]),\"features\")\n",
    "print(\"Number of data clusters\",len(train_features), \"Number of data points in test data\", len(test_features[0]), \"Each data point contains\", len(test_features[0][0]),\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting first sizeof_train_data timestamp values i.e 70% of 13099 (total timestamps) for our training data\n",
    "\n",
    "tsne_train_flat_PULocationID = [i[:sizeof_train_data] for i in tsne_PULocationID]\n",
    "tsne_train_flat_Tip_amount = [i[:sizeof_train_data] for i in predict_list_tip]\n",
    "tsne_train_flat_weekday = [i[:sizeof_train_data] for i in tsne_weekday]\n",
    "tsne_train_flat_output = [i[:sizeof_train_data] for i in output]\n",
    "tsne_train_flat_output_tip = [i[:sizeof_train_data] for i in output_tip]\n",
    "tsne_train_flat_exp_avg = [i[:sizeof_train_data] for i in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the rest of the timestamp values i.e 30% of sizeof_train_data + sizeof_test_data (total timestamps) for our test data\n",
    "\n",
    "tsne_test_flat_PULocationID = [i[sizeof_train_data:] for i in tsne_PULocationID]\n",
    "tsne_test_flat_Tip_amount = [i[sizeof_train_data:] for i in predict_list_tip]\n",
    "tsne_test_flat_weekday = [i[sizeof_train_data:] for i in tsne_weekday]\n",
    "tsne_test_flat_output = [i[sizeof_train_data:] for i in output]\n",
    "tsne_test_flat_output_tip = [i[sizeof_train_data:] for i in output_tip]\n",
    "tsne_test_flat_exp_avg = [i[sizeof_train_data:] for i in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above contains values in the form of list of lists (i.e. list of values of each region), here we make all of them in one list\n",
    "train_new_features = []\n",
    "for i in range(0,nyc_regions_cnt):\n",
    "    train_new_features.extend(train_features[i])\n",
    "test_new_features = []\n",
    "for i in range(0,nyc_regions_cnt):\n",
    "    test_new_features.extend(test_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_train_PULocationID = sum(tsne_train_flat_PULocationID, [])\n",
    "tsne_train_Tip_amount = sum(tsne_train_flat_Tip_amount, [])\n",
    "tsne_train_weekday = sum(tsne_train_flat_weekday, [])\n",
    "tsne_train_output = sum(tsne_train_flat_output, [])\n",
    "tsne_train_output_tip = sum(tsne_train_flat_output_tip, [])\n",
    "tsne_train_exp_avg = sum(tsne_train_flat_exp_avg,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_test_PULocationID = sum(tsne_test_flat_PULocationID, [])\n",
    "tsne_test_Tip_amount = sum(tsne_test_flat_Tip_amount, [])\n",
    "tsne_test_weekday = sum(tsne_test_flat_weekday, [])\n",
    "tsne_test_output = sum(tsne_test_flat_output, [])\n",
    "tsne_test_output_tip = sum(tsne_test_flat_output_tip, [])\n",
    "tsne_test_exp_avg = sum(tsne_test_flat_exp_avg,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215349, 9)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data frame for our train data\n",
    "columns = ['ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "df_train = pd.DataFrame(data=train_new_features, columns=columns) \n",
    "# df_train['lat'] = tsne_train_lat\n",
    "# df_train['lon'] = tsne_train_lon\n",
    "\n",
    "df_train['PULocationID'] = tsne_train_PULocationID\n",
    "df_train['Tip_amount'] = tsne_train_Tip_amount\n",
    "df_train['weekday'] = tsne_train_weekday\n",
    "df_train['exp_avg'] = tsne_train_exp_avg\n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92322, 9)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data frame for our train data\n",
    "df_test = pd.DataFrame(data=test_new_features, columns=columns) \n",
    "# df_test['lat'] = tsne_test_lat\n",
    "# df_test['lon'] = tsne_test_lon\n",
    "\n",
    "df_test['PULocationID'] = tsne_test_PULocationID\n",
    "df_test['Tip_amount'] = tsne_test_Tip_amount\n",
    "df_test['weekday'] = tsne_test_weekday\n",
    "df_test['exp_avg'] = tsne_test_exp_avg\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find more about LinearRegression function here http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
    "\n",
    "# some of methods of LinearRegression()\n",
    "# fit(X, y[, sample_weight])\tFit linear model.\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict using the linear model\n",
    "# score(X, y[, sample_weight])\tReturns the coefficient of determination R^2 of the prediction.\n",
    "# set_params(**params)\tSet the parameters of this estimator.\n",
    "# -----------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1-2-copy-8/\n",
    "# -----------------------\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_reg=LinearRegression().fit(df_train, tsne_train_output)\n",
    "\n",
    "y_pred = lr_reg.predict(df_test)\n",
    "lr_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = lr_reg.predict(df_train)\n",
    "lr_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_features=&#x27;sqrt&#x27;, min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_features=&#x27;sqrt&#x27;, min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_features='sqrt', min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned random forest regressor on our train data\n",
    "# find more about LinearRegression function here http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion=’mse’, max_depth=None, min_samples_split=2, \n",
    "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# apply(X)\tApply trees in the forest to X, return leaf indices.\n",
    "# decision_path(X)\tReturn the decision path in the forest\n",
    "# fit(X, y[, sample_weight])\tBuild a forest of trees from the training set (X, y).\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict regression target for X.\n",
    "# score(X, y[, sample_weight])\tReturns the coefficient of determination R^2 of the prediction.\n",
    "# -----------------------\n",
    "# video link1: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/regression-using-decision-trees-2/\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "regr1 = RandomForestRegressor(max_features='sqrt',min_samples_leaf=4,min_samples_split=3,n_estimators=40, n_jobs=-1)\n",
    "regr1.fit(df_train, tsne_train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data using our trained random forest model \n",
    "\n",
    "# the models regr1 is already hyper parameter tuned\n",
    "# the parameters that we got above are found using grid search\n",
    "\n",
    "y_pred = regr1.predict(df_test)\n",
    "rndf_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = regr1.predict(df_train)\n",
    "rndf_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ft_5', 'ft_4', 'ft_3', 'ft_2', 'ft_1', 'PULocationID', 'Tip_amount',\n",
      "       'weekday', 'exp_avg'],\n",
      "      dtype='object')\n",
      "[0.01527355 0.04986627 0.10413751 0.22040291 0.2385629  0.0040881\n",
      " 0.0150577  0.0022944  0.35031666]\n"
     ]
    }
   ],
   "source": [
    "#feature importances based on analysis using random forest\n",
    "print (df_train.columns)\n",
    "print (regr1.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using XgBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "             random_state=0, reg_alpha=200, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "             random_state=0, reg_alpha=200, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor='auto',\n",
       "             random_state=0, reg_alpha=200, ...)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned Xg-Boost regressor on our train data\n",
    "\n",
    "# find more about XGBRegressor function here http://xgboost.readthedocs.io/en/latest/python/python_api.html?#module-xgboost.sklearn\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# xgboost.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', \n",
    "# booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, \n",
    "# colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, \n",
    "# missing=None, **kwargs)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(data, output_margin=False, ntree_limit=0) : Predict with data. NOTE: This function is not thread safe.\n",
    "# get_score(importance_type='weight') -> get the feature importance\n",
    "# -----------------------\n",
    "# video link1: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/regression-using-decision-trees-2/\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "x_model = xgb.XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=3,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " reg_alpha=200, reg_lambda=200,\n",
    " colsample_bytree=0.8,nthread=4)\n",
    "x_model.fit(df_train, tsne_train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting with our trained Xg-Boost regressor\n",
    "# the models x_model is already hyper parameter tuned\n",
    "# the parameters that we got above are found using grid search\n",
    "\n",
    "y_pred = x_model.predict(df_test)\n",
    "xgb_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = x_model.predict(df_train)\n",
    "xgb_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft_5</th>\n",
       "      <th>ft_4</th>\n",
       "      <th>ft_3</th>\n",
       "      <th>ft_2</th>\n",
       "      <th>ft_1</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>Tip_amount</th>\n",
       "      <th>weekday</th>\n",
       "      <th>exp_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.806325</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.266009</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.399257</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1.776162</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2.388549</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215344</th>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>69</td>\n",
       "      <td>1.125631</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215345</th>\n",
       "      <td>47</td>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>58</td>\n",
       "      <td>69</td>\n",
       "      <td>1.063396</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215346</th>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>69</td>\n",
       "      <td>1.192769</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215347</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>49</td>\n",
       "      <td>69</td>\n",
       "      <td>1.262974</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215348</th>\n",
       "      <td>48</td>\n",
       "      <td>58</td>\n",
       "      <td>56</td>\n",
       "      <td>49</td>\n",
       "      <td>53</td>\n",
       "      <td>69</td>\n",
       "      <td>1.079552</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215349 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ft_5  ft_4  ft_3  ft_2  ft_1  PULocationID  Tip_amount  weekday  \\\n",
       "0         24     1     1     4    11             1    1.806325        2   \n",
       "1          1     1     4    11     9             1    1.266009        2   \n",
       "2          1     4    11     9    11             1    1.399257        2   \n",
       "3          4    11     9    11    13             1    1.776162        2   \n",
       "4         11     9    11    13    10             1    2.388549        2   \n",
       "...      ...   ...   ...   ...   ...           ...         ...      ...   \n",
       "215344    44    47    42    36    48            69    1.125631        2   \n",
       "215345    47    42    36    48    58            69    1.063396        2   \n",
       "215346    42    36    48    58    56            69    1.192769        2   \n",
       "215347    36    48    58    56    49            69    1.262974        2   \n",
       "215348    48    58    56    49    53            69    1.079552        2   \n",
       "\n",
       "        exp_avg  \n",
       "0             8  \n",
       "1             8  \n",
       "2            10  \n",
       "3            12  \n",
       "4            10  \n",
       "...         ...  \n",
       "215344       44  \n",
       "215345       53  \n",
       "215346       55  \n",
       "215347       50  \n",
       "215348       52  \n",
       "\n",
       "[215349 rows x 9 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0344444444444445,\n",
       " 1.4563636363636363,\n",
       " 1.9376923076923076,\n",
       " 2.651,\n",
       " 1.6978947368421051,\n",
       " 2.1854545454545455,\n",
       " 2.930625,\n",
       " 2.4076470588235295,\n",
       " 2.0195238095238093,\n",
       " 1.6878947368421053,\n",
       " 1.9994736842105265,\n",
       " 1.139,\n",
       " 2.2239999999999998,\n",
       " 2.9709090909090907,\n",
       " 1.9922222222222221,\n",
       " 1.5235714285714284,\n",
       " 2.147692307692308,\n",
       " 2.234375,\n",
       " 1.2825,\n",
       " 1.3807142857142856,\n",
       " 1.4080000000000001,\n",
       " 1.358,\n",
       " 2.131818181818182,\n",
       " 2.26625,\n",
       " 2.4739999999999998,\n",
       " 1.745,\n",
       " 0.3333333333333333,\n",
       " 1.2871428571428571,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1.9525000000000001,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 2.83,\n",
       " 0.0,\n",
       " 2.66,\n",
       " 0.0,\n",
       " 1.5,\n",
       " 2.925,\n",
       " 1.96,\n",
       " 1.5983333333333334,\n",
       " 1.3633333333333333,\n",
       " 0.6666666666666666,\n",
       " 1.4866666666666666,\n",
       " 3.2133333333333334,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.43714285714285717,\n",
       " 1.3333333333333333,\n",
       " 2.33,\n",
       " 9.6,\n",
       " 1.5866666666666667,\n",
       " 2.05,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2.15,\n",
       " 0.7166666666666667,\n",
       " 0.3875,\n",
       " 0.25,\n",
       " 1.045,\n",
       " 0.86,\n",
       " 0.8866666666666667,\n",
       " 1.38,\n",
       " 1.175,\n",
       " 1.61,\n",
       " 1.5,\n",
       " 2.12,\n",
       " 0.0,\n",
       " 0.525,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1.502,\n",
       " 0.58,\n",
       " 3.58,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2.0,\n",
       " 1.71,\n",
       " 0.8700000000000001,\n",
       " 0.7,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.0,\n",
       " 1.9449999999999998,\n",
       " 2.45,\n",
       " 0.0,\n",
       " 2.08,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.0,\n",
       " 1.71,\n",
       " 1.4333333333333336,\n",
       " 2.26,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2.425,\n",
       " 0.0,\n",
       " 1.25,\n",
       " 2.205,\n",
       " 2.82,\n",
       " 1.7375,\n",
       " 1.66,\n",
       " 0.75,\n",
       " 1.175,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2.1,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.68,\n",
       " 1.5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2.235,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1.58,\n",
       " 0.0,\n",
       " 1.22,\n",
       " 1.19,\n",
       " 0.3333333333333333,\n",
       " 0.3333333333333333,\n",
       " 1.2,\n",
       " 0.3333333333333333,\n",
       " 1.275,\n",
       " 2.38,\n",
       " 0.6533333333333333,\n",
       " 1.022,\n",
       " 1.0775,\n",
       " 1.442,\n",
       " 1.378,\n",
       " 0.41500000000000004,\n",
       " 2.0500000000000003,\n",
       " 1.9466666666666665,\n",
       " 1.316,\n",
       " 2.3857142857142857,\n",
       " 0.812,\n",
       " 1.075,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.6033333333333335,\n",
       " 1.2866666666666666,\n",
       " 1.0,\n",
       " 2.06,\n",
       " 1.1328571428571428,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 2.0300000000000002,\n",
       " 2.645,\n",
       " 1.17,\n",
       " 0.64,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.4375,\n",
       " 1.665,\n",
       " 0.9866666666666667,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.85,\n",
       " 1.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 1.15,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 1.5,\n",
       " 0.0,\n",
       " 1.192,\n",
       " 0.375,\n",
       " 0.64,\n",
       " 2.05,\n",
       " 1.4100000000000001,\n",
       " 0.0,\n",
       " 1.3366666666666667,\n",
       " 0.45333333333333337,\n",
       " 0.99,\n",
       " 0.98,\n",
       " 3.16,\n",
       " 0.6833333333333332,\n",
       " 0.83,\n",
       " 1.9675,\n",
       " 0.5499999999999999,\n",
       " 0.8,\n",
       " 0.0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.58,\n",
       " 0.0,\n",
       " 1.6500000000000001,\n",
       " 0.0,\n",
       " 1.5,\n",
       " 1.36,\n",
       " 0.6166666666666667,\n",
       " 3.35,\n",
       " 0.0,\n",
       " 1.2275,\n",
       " 0.0,\n",
       " 0.39,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1.0,\n",
       " 2.33,\n",
       " 0.675,\n",
       " 0.7866666666666666,\n",
       " 0.3333333333333333,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.85,\n",
       " 0.0,\n",
       " 1.06,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.6100000000000003,\n",
       " 0.9275,\n",
       " 2.08,\n",
       " 0.3333333333333333,\n",
       " 0.9560000000000001,\n",
       " 0.8380000000000001,\n",
       " 2.914,\n",
       " 3.75,\n",
       " 0.0,\n",
       " 0.8559999999999999,\n",
       " 0.44333333333333336,\n",
       " 2.3939999999999997,\n",
       " 0.33,\n",
       " 2.0733333333333337,\n",
       " 1.0,\n",
       " 1.3499999999999999,\n",
       " 1.43,\n",
       " 0.9258333333333333,\n",
       " 1.8628571428571428,\n",
       " 2.694,\n",
       " 0.3333333333333333,\n",
       " 0.44,\n",
       " 2.2733333333333334,\n",
       " 1.6800000000000002,\n",
       " 0.36000000000000004,\n",
       " 0.22666666666666668,\n",
       " 0.3,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5171428571428571,\n",
       " 1.04,\n",
       " 0.0,\n",
       " 0.7533333333333333,\n",
       " 0.0,\n",
       " 1.61,\n",
       " 1.0,\n",
       " 1.58,\n",
       " 2.06,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.9299999999999999,\n",
       " 3.3333333333333335,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1.98,\n",
       " 5.0,\n",
       " 0.5720000000000001,\n",
       " 0.9,\n",
       " 0.94,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1.1583333333333332,\n",
       " 2.64,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.7,\n",
       " 0.7533333333333333,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.7833333333333333,\n",
       " 2.14,\n",
       " 0.0,\n",
       " 1.8033333333333335,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.3333333333333333,\n",
       " 0.0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.49,\n",
       " 2.12,\n",
       " 0.6900000000000001,\n",
       " 1.66,\n",
       " 0.0,\n",
       " 1.534,\n",
       " 0.5,\n",
       " 1.3399999999999999,\n",
       " 0.4625,\n",
       " 1.6733333333333331,\n",
       " 1.23,\n",
       " 2.125,\n",
       " 1.74,\n",
       " 0.87,\n",
       " 1.8775,\n",
       " 3.0,\n",
       " 1.05,\n",
       " 2.75,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0439999999999996,\n",
       " 4.086666666666667,\n",
       " 0.7150000000000001,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1.725,\n",
       " 1.0,\n",
       " 2.0,\n",
       " 1.33,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1.46,\n",
       " 3.1550000000000002,\n",
       " 0.0,\n",
       " 0.36875,\n",
       " 0.5,\n",
       " 1.228,\n",
       " 2.317142857142857,\n",
       " 2.948,\n",
       " 0.954,\n",
       " 1.67,\n",
       " 0.9833333333333334,\n",
       " 1.6171428571428572,\n",
       " 0.5,\n",
       " 2.37,\n",
       " 1.3439999999999999,\n",
       " 1.8699999999999999,\n",
       " 0.5833333333333334,\n",
       " 1.525,\n",
       " 1.32,\n",
       " 0.5666666666666667,\n",
       " 1.1,\n",
       " 1.09,\n",
       " 5.855,\n",
       " 1.0916666666666666,\n",
       " 0.58,\n",
       " 2.2119999999999997,\n",
       " 4.95,\n",
       " 1.05,\n",
       " 1.73,\n",
       " 2.566666666666667,\n",
       " 0.16666666666666666,\n",
       " 1.1833333333333333,\n",
       " 0.5,\n",
       " 0.9,\n",
       " 2.434,\n",
       " 0.6866666666666666,\n",
       " 0.7275,\n",
       " 1.3175,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.3366666666666667,\n",
       " 0.83,\n",
       " 0.35333333333333333,\n",
       " 0.89,\n",
       " 0.35000000000000003,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.625,\n",
       " 2.14,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2.56,\n",
       " 1.56,\n",
       " 1.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.989999999999999,\n",
       " 0.8049999999999999,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.96,\n",
       " 0.0,\n",
       " 0.264,\n",
       " 2.103333333333333,\n",
       " 1.5,\n",
       " 0.975,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.65,\n",
       " 1.425,\n",
       " 0.6583333333333333,\n",
       " 0.892,\n",
       " 0.8275,\n",
       " 1.5616666666666668,\n",
       " 1.7225,\n",
       " 1.02,\n",
       " 1.5233333333333334,\n",
       " 1.6800000000000002,\n",
       " 1.2016666666666667,\n",
       " 0.7566666666666667,\n",
       " 1.5699999999999998,\n",
       " 1.6525,\n",
       " 1.1388888888888888,\n",
       " 1.7730000000000001,\n",
       " 0.7155555555555555,\n",
       " 1.72,\n",
       " 1.1571428571428573,\n",
       " 2.415,\n",
       " 1.5657142857142858,\n",
       " 2.19,\n",
       " 1.2774999999999999,\n",
       " 0.0,\n",
       " 2.055,\n",
       " 1.33,\n",
       " 1.606153846153846,\n",
       " 2.27,\n",
       " 2.3850000000000002,\n",
       " 1.8908333333333334,\n",
       " 2.6675,\n",
       " 1.177,\n",
       " 1.6923529411764706,\n",
       " 1.170909090909091,\n",
       " 1.9033333333333333,\n",
       " 2.3141666666666665,\n",
       " 1.5816666666666668,\n",
       " 1.6607692307692308,\n",
       " 1.8258333333333334,\n",
       " 1.9271428571428573,\n",
       " 1.0742857142857143,\n",
       " 1.7614285714285713,\n",
       " 2.4659999999999997,\n",
       " 2.39,\n",
       " 1.8871428571428572,\n",
       " 0.695,\n",
       " 1.582,\n",
       " 1.705,\n",
       " 1.0625,\n",
       " 1.5383333333333333,\n",
       " 1.6666666666666667,\n",
       " 1.55,\n",
       " 0.7922222222222222,\n",
       " 1.58,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2.93,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1.33,\n",
       " 0.2875,\n",
       " 0.0,\n",
       " 2.47,\n",
       " 0.8533333333333334,\n",
       " 0.0,\n",
       " 0.5166666666666667,\n",
       " 0.52,\n",
       " 0.0,\n",
       " 1.34,\n",
       " 0.825,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.44,\n",
       " 0.7166666666666667,\n",
       " 0.0,\n",
       " 1.0583333333333333,\n",
       " 2.365,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1.30875,\n",
       " 1.84,\n",
       " 1.0,\n",
       " 0.7142857142857143,\n",
       " 1.64,\n",
       " 0.0,\n",
       " 0.78,\n",
       " 0.7925,\n",
       " 0.51,\n",
       " 1.4675,\n",
       " 1.1357142857142857,\n",
       " 0.0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1.695,\n",
       " 0.0,\n",
       " 1.3379999999999999,\n",
       " 1.3033333333333335,\n",
       " 2.004,\n",
       " 0.78,\n",
       " 0.0,\n",
       " 0.8433333333333334,\n",
       " 2.1199999999999997,\n",
       " 1.3533333333333335,\n",
       " 0.0,\n",
       " 0.63,\n",
       " 0.6666666666666666,\n",
       " 0.348,\n",
       " 1.3533333333333333,\n",
       " 1.022857142857143,\n",
       " 1.15,\n",
       " 0.6666666666666666,\n",
       " 3.6475,\n",
       " 0.922,\n",
       " 1.025,\n",
       " 0.6916666666666668,\n",
       " 1.23,\n",
       " 1.3375,\n",
       " 3.13,\n",
       " 2.3666666666666667,\n",
       " 0.82,\n",
       " 2.83,\n",
       " 1.5899999999999999,\n",
       " 2.27,\n",
       " 1.4733333333333334,\n",
       " 0.9244444444444445,\n",
       " 0.98,\n",
       " 2.06,\n",
       " 3.1566666666666663,\n",
       " 2.11,\n",
       " 0.2,\n",
       " 0.875,\n",
       " 0.6225,\n",
       " 1.0666666666666667,\n",
       " 1.3314285714285714,\n",
       " 1.4040000000000001,\n",
       " 1.482,\n",
       " 1.0266666666666666,\n",
       " 1.92,\n",
       " 3.44625,\n",
       " 1.8025,\n",
       " 1.5175,\n",
       " 1.2633333333333334,\n",
       " 0.9,\n",
       " 0.7244444444444444,\n",
       " 1.6325,\n",
       " 1.6925,\n",
       " 2.814285714285714,\n",
       " 0.5871428571428571,\n",
       " 0.9014285714285715,\n",
       " 0.0,\n",
       " 3.335,\n",
       " 2.14625,\n",
       " 0.49333333333333335,\n",
       " 3.33875,\n",
       " 2.5300000000000002,\n",
       " 1.391,\n",
       " 1.6190000000000002,\n",
       " 0.9833333333333333,\n",
       " 1.773125,\n",
       " 2.420769230769231,\n",
       " 1.1290909090909091,\n",
       " 1.5473333333333334,\n",
       " 1.6009999999999998,\n",
       " 1.1933333333333334,\n",
       " 1.425,\n",
       " 3.0014285714285713,\n",
       " 1.5585714285714285,\n",
       " 2.26625,\n",
       " 3.2084615384615387,\n",
       " 1.948,\n",
       " 2.4855555555555555,\n",
       " 1.915,\n",
       " 1.80875,\n",
       " 1.8484615384615386,\n",
       " 1.9121052631578948,\n",
       " 2.6662500000000002,\n",
       " 2.279230769230769,\n",
       " 1.1942857142857142,\n",
       " 1.4516666666666669,\n",
       " 2.032857142857143,\n",
       " 1.204,\n",
       " 1.18,\n",
       " 0.8780000000000001,\n",
       " 2.66,\n",
       " 1.94,\n",
       " 1.3657142857142859,\n",
       " 0.0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1.26,\n",
       " 1.275,\n",
       " 0.39999999999999997,\n",
       " 2.51,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.0,\n",
       " 1.0199999999999998,\n",
       " 2.1675,\n",
       " 2.063333333333333,\n",
       " 1.0,\n",
       " 1.715,\n",
       " 2.012,\n",
       " 1.4425,\n",
       " 0.865,\n",
       " 1.03,\n",
       " 2.515,\n",
       " 0.65,\n",
       " 0.915,\n",
       " 0.9259999999999999,\n",
       " 0.0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.3333333333333333,\n",
       " 0.62,\n",
       " 0.0,\n",
       " 1.4366666666666668,\n",
       " 0.0,\n",
       " 0.5625,\n",
       " 1.6600000000000001,\n",
       " 0.7200000000000001,\n",
       " 2.46,\n",
       " 1.305,\n",
       " 0.78,\n",
       " 1.16,\n",
       " 0.3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 1.76,\n",
       " 1.7425,\n",
       " 0.0,\n",
       " 1.75,\n",
       " 1.54,\n",
       " 4.6850000000000005,\n",
       " 1.0533333333333335,\n",
       " 0.38,\n",
       " 3.45,\n",
       " 0.0,\n",
       " 0.66,\n",
       " 2.26,\n",
       " 1.335,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.88,\n",
       " 1.545,\n",
       " 0.725,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.175,\n",
       " 0.7200000000000001,\n",
       " 0.64,\n",
       " 1.375,\n",
       " 1.6266666666666667,\n",
       " 2.2619999999999996,\n",
       " 1.8533333333333333,\n",
       " 2.1025,\n",
       " 0.63,\n",
       " 0.0,\n",
       " 11.81,\n",
       " 0.0,\n",
       " 7.45,\n",
       " 0.0,\n",
       " 1.722,\n",
       " 1.095,\n",
       " 3.35,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.355,\n",
       " 1.8199999999999996,\n",
       " 0.0,\n",
       " 2.995,\n",
       " 2.14,\n",
       " 3.1449999999999996,\n",
       " 0.0,\n",
       " 1.5533333333333335,\n",
       " 0.0,\n",
       " 1.36,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2.0275,\n",
       " 1.05,\n",
       " 1.7462499999999999,\n",
       " 1.6025,\n",
       " 0.55,\n",
       " 0.612,\n",
       " 1.7771428571428574,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1.06,\n",
       " 1.7766666666666666,\n",
       " 1.46,\n",
       " 2.0700000000000003,\n",
       " 1.6311111111111112,\n",
       " 1.4,\n",
       " 2.58,\n",
       " 2.35,\n",
       " 1.26,\n",
       " 2.322,\n",
       " 1.22,\n",
       " 1.7875,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 0.255,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.0,\n",
       " 0.83,\n",
       " 1.42,\n",
       " 1.25,\n",
       " 0.0,\n",
       " 1.15,\n",
       " 0.79,\n",
       " 1.46,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1.81,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 2.22,\n",
       " 1.55,\n",
       " 0.3333333333333333,\n",
       " 3.0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.9633333333333334,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0.0,\n",
       " 6.54,\n",
       " 1.0,\n",
       " 0.87,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 1.1199999999999999,\n",
       " 1.4120000000000001,\n",
       " 0.0,\n",
       " 6.710000000000001,\n",
       " 2.445,\n",
       " 3.3333333333333335,\n",
       " 0.7775000000000001,\n",
       " 1.748,\n",
       " 1.8250000000000002,\n",
       " 2.16,\n",
       " 1.85,\n",
       " 1.15,\n",
       " 1.8,\n",
       " 1.05,\n",
       " 2.8,\n",
       " 1.32,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2.0475,\n",
       " 2.3600000000000003,\n",
       " 1.2366666666666666,\n",
       " 1.8233333333333333,\n",
       " 1.0,\n",
       " 1.86,\n",
       " 1.525,\n",
       " 2.005,\n",
       " 1.75,\n",
       " ...]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_train_output_tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ft_5': 842.0,\n",
       " 'ft_4': 685.0,\n",
       " 'ft_3': 817.0,\n",
       " 'ft_2': 950.0,\n",
       " 'ft_1': 1055.0,\n",
       " 'PULocationID': 761.0,\n",
       " 'Tip_amount': 722.0,\n",
       " 'weekday': 226.0,\n",
       " 'exp_avg': 702.0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature importances\n",
    "x_model.get_booster().get_score(importance_type=\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the error metric values for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_R2=[]\n",
    "test_R2=[]\n",
    "\n",
    "train_R2.append(r2_score(tsne_train_output,df_train['ft_1'].values))\n",
    "train_R2.append(r2_score(tsne_train_output,df_train['exp_avg'].values))\n",
    "train_R2.append(r2_score(tsne_train_output,rndf_train_predictions))\n",
    "train_R2.append(r2_score(tsne_train_output,xgb_train_predictions))\n",
    "train_R2.append(r2_score(tsne_train_output,lr_train_predictions))\n",
    "train_R2.append(r2_score(tsne_train_output_tip,df_train['Tip_amount'].values))\n",
    "\n",
    "test_R2.append(r2_score(tsne_test_output,df_test['ft_1'].values))\n",
    "test_R2.append(r2_score(tsne_test_output,df_test['exp_avg'].values))\n",
    "test_R2.append(r2_score(tsne_test_output,rndf_test_predictions))\n",
    "test_R2.append(r2_score(tsne_test_output,xgb_test_predictions))\n",
    "test_R2.append(r2_score(tsne_test_output,lr_test_predictions))\n",
    "test_R2.append(r2_score(tsne_test_output_tip,df_test['Tip_amount'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 1 ~ 1 Manhattan R2\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Baseline Model\t\t\t\tTrain: \t0.9275364793206575\tTest: \t0.9312412872265359\n",
      "Exponential Averages Forecasting\tTrain: \t0.9361166759474355\tTest: \t0.9400040382885209\n",
      "Random Forest Regression\t\tTrain: \t0.9695759719040822\tTest: \t0.9429384790454273\n",
      "XgBoost Regression\t\t\tTrain: \t0.9435557112890061\tTest: \t0.9437120898550438\n",
      "Linear Regression\t\t\tTrain: \t0.9378369832128254\tTest: \t0.9415483950787751\n",
      "Baseline Model of Tip\t\t\tTrain: \t-0.030201990127818323\tTest: \t0.02006365706213742\n"
     ]
    }
   ],
   "source": [
    "print(f\"{base_year+1} 1 ~ {base_month_count} {region} R2\")\n",
    "print (\"--------------------------------------------------------------------------------------------------------\")\n",
    "print (\"Baseline Model\\t\\t\\t\",\"Train: \",train_R2[0],\"Test: \",test_R2[0],sep='\\t')\n",
    "print (\"Exponential Averages Forecasting\",\"Train: \",train_R2[1],\"Test: \",test_R2[1],sep='\\t')\n",
    "print (\"Random Forest Regression\\t\",\"Train: \",train_R2[2],\"Test: \",test_R2[2],sep='\\t')\n",
    "print (\"XgBoost Regression\\t\\t\",\"Train: \",train_R2[3],\"Test: \",test_R2[3],sep='\\t')\n",
    "print (\"Linear Regression\\t\\t\",\"Train: \",train_R2[4],\"Test: \",test_R2[4],sep='\\t')\n",
    "print (\"Baseline Model of Tip\\t\\t\",\"Train: \",train_R2[5],\"Test: \",test_R2[5],sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MAE=[]\n",
    "test_MAE=[]\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,df_train['ft_1'].values))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,df_train['exp_avg'].values))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,rndf_train_predictions))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,xgb_train_predictions))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,lr_train_predictions))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,df_train['Tip_amount'].values))\n",
    "\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,df_test['ft_1'].values))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,df_test['exp_avg'].values))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,rndf_test_predictions))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,xgb_test_predictions))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,lr_test_predictions))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,df_test['Tip_amount'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 1 ~ 1 Manhattan MAE\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Baseline Model\t\t\t\tTrain: \t4.376946259327882\tTest: \t4.48041636879617\n",
      "Exponential Averages Forecasting\tTrain: \t4.0952500359880935\tTest: \t4.183076623123416\n",
      "Random Forest Regression\t\tTrain: \t2.8067509020241563\tTest: \t4.179220554147441\n",
      "XgBoost Regression\t\t\tTrain: \t3.93488244663314\tTest: \t4.153636186391109\n",
      "Linear Regression\t\t\tTrain: \t4.0569494169928815\tTest: \t4.1414830701241305\n",
      "Baseline Model of Tip\t\t\tTrain: \t20.93751226296253\tTest: \t22.493143562747775\n"
     ]
    }
   ],
   "source": [
    "print(f\"{base_year+1} 1 ~ {base_month_count} {region} MAE\")\n",
    "print (\"--------------------------------------------------------------------------------------------------------\")\n",
    "print (\"Baseline Model\\t\\t\\t\",\"Train: \",train_MAE[0],\"Test: \",test_MAE[0],sep='\\t')\n",
    "print (\"Exponential Averages Forecasting\",\"Train: \",train_MAE[1],\"Test: \",test_MAE[1],sep='\\t')\n",
    "print (\"Random Forest Regression\\t\",\"Train: \",train_MAE[2],\"Test: \",test_MAE[2],sep='\\t')\n",
    "print (\"XgBoost Regression\\t\\t\",\"Train: \",train_MAE[3],\"Test: \",test_MAE[3],sep='\\t')\n",
    "print (\"Linear Regression\\t\\t\",\"Train: \",train_MAE[4],\"Test: \",test_MAE[4],sep='\\t')\n",
    "print (\"Baseline Model of Tip\\t\\t\",\"Train: \",train_MAE[5],\"Test: \",test_MAE[5],sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a4346d64cb42476a551e7595e0385aacee0262d177edc52296f761023fa4e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
