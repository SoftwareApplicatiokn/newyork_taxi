{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unix time: https://www.unixtimestamp.com/\n",
    "import datetime  # Convert to unix time\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time  # Convert to unix time\n",
    "import warnings\n",
    "\n",
    "import dask.dataframe as dd  # similar to pandas\n",
    "import matplotlib.pylab as plt\n",
    "# if numpy is not installed already : pip3 install numpy\n",
    "import numpy as np  # Do aritmetic operations on arrays\n",
    "import pandas as pd  # pandas to create small dataframes\n",
    "import seaborn as sns  # Plots\n",
    "# to install xgboost: pip3 install xgboost\n",
    "# if it didnt happen check install_xgboost.JPG\n",
    "import xgboost as xgb\n",
    "from matplotlib import rcParams  # Size of plots\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans  # Clustering\n",
    "# to install sklearn: pip install -U scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (mean_absolute_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_squared_error, r2_score)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_year = 2019\n",
    "base_month_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    months_frame = []\n",
    "    months_groupby = []\n",
    "    for i in range(1,base_month_count+1):\n",
    "        tmp_frame = pd.read_parquet(f'preprocessing_yellow_tripdata_{base_year+1}_{i}.parquet',engine='pyarrow')\n",
    "        \n",
    "        tmp_groupby = tmp_frame[['PULocationID','pickup_bins','trip_distance']].groupby(['PULocationID','pickup_bins'], dropna=False).count()\n",
    "        tmp_tip_amount = tmp_frame[['PULocationID','pickup_bins','tip_amount']].groupby(['PULocationID','pickup_bins'], dropna=False).mean()\n",
    "        #print(tmp_groupby)\n",
    "        # print(\"-------------------------\")\n",
    "        #print(tmp_tip_amount)\n",
    "        # print(\"-------------------------\")\n",
    "        #tmp_groupby = pd.concat([tmp_trip_distance, tmp_tip_amount])\n",
    "        #print(tmp_groupby)\n",
    "        tmp_groupby = pd.merge(tmp_groupby, tmp_tip_amount, on = ['PULocationID','pickup_bins'], how = \"left\")\n",
    "\n",
    "        \n",
    "        print(tmp_groupby)\n",
    "        months_frame.append(tmp_frame)\n",
    "        months_groupby.append(tmp_groupby)\n",
    "    return months_frame, months_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          trip_distance  tip_amount\n",
      "PULocationID pickup_bins                           \n",
      "4            1                        5    3.550000\n",
      "             2                        2    3.005000\n",
      "             3                        5    3.054000\n",
      "             4                       12    4.261667\n",
      "             5                       10    2.359000\n",
      "...                                 ...         ...\n",
      "263          4459                    27    2.472222\n",
      "             4460                    17    2.577647\n",
      "             4461                    23    3.210000\n",
      "             4462                    25    2.796800\n",
      "             4463                    26    2.469615\n",
      "\n",
      "[225377 rows x 2 columns]\n",
      "                          trip_distance  tip_amount\n",
      "PULocationID pickup_bins                           \n",
      "4            0                        7    3.347143\n",
      "             1                       11    3.591818\n",
      "             2                       13    3.283077\n",
      "             3                       12    2.703333\n",
      "             4                        7    2.682857\n",
      "...                                 ...         ...\n",
      "263          4172                    25    2.485200\n",
      "             4173                    29    2.265862\n",
      "             4174                    30    2.384667\n",
      "             4175                    31    2.838710\n",
      "             4176                     1    4.820000\n",
      "\n",
      "[213249 rows x 2 columns]\n",
      "                          trip_distance  tip_amount\n",
      "PULocationID pickup_bins                           \n",
      "4            -8                       1    1.000000\n",
      "              0                      13    1.900000\n",
      "              1                      21    2.582381\n",
      "              2                      18    2.596111\n",
      "              3                      18    3.391667\n",
      "...                                 ...         ...\n",
      "263           4453                    1    3.000000\n",
      "              4458                    1    3.000000\n",
      "              4461                    5    2.502000\n",
      "              4463                    1    1.000000\n",
      "              8012                    1    2.160000\n",
      "\n",
      "[165036 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "months_frame, months_groupby = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴욕 지역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_df = pd.read_csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"Manhattan\"\n",
    "nyc_region = taxi_zone_df[taxi_zone_df['Borough'] == region]\n",
    "nyc_region_number = nyc_region['LocationID']\n",
    "nyc_regions_cnt = len(nyc_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464\n"
     ]
    }
   ],
   "source": [
    "# number of 10min indices for jan 2019= 24*31*60/10 = max_pickup_bins_len\n",
    "interval = 10\n",
    "days = [31,29,31,30,31,30,31,31,30,31,30,31]\n",
    "pickup_bins_len = []\n",
    "\n",
    "for day in days:\n",
    "    pickup_bins_len.append(int(24*60*day/interval))\n",
    "max_pickup_bins_len = max(pickup_bins_len)\n",
    "print(max_pickup_bins_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills a value of zero for every bin where no pickup data is present \n",
    "# the count_values: number pickps that are happened in each region for each 10min intravel\n",
    "# there wont be any value if there are no picksups.\n",
    "# values: number of unique bins\n",
    "\n",
    "# for every 10min intravel(pickup_bin) we will check it is there in our unique bin,\n",
    "# if it is there we will add the count_values[index] to smoothed data\n",
    "# if not we add smoothed data (which is calculated based on the methods that are discussed in the above markdown cell)\n",
    "# we finally return smoothed data\n",
    "def smoothing(count_values,values):\n",
    "    smoothed_regions=[] # stores list of final smoothed values of each reigion\n",
    "    ind=0\n",
    "    repeat=0 \n",
    "    smoothed_value=0\n",
    "    for r in range(1,nyc_regions_cnt+1):\n",
    "        smoothed_bins=[] #stores the final smoothed values\n",
    "        repeat=0\n",
    "        for i in range(max_pickup_bins_len):\n",
    "            if repeat!=0: # prevents iteration for a value which is already visited/resolved\n",
    "                repeat-=1\n",
    "                continue\n",
    "            if i in values[r-1]: #checks if the pickup-bin exists \n",
    "                smoothed_bins.append(count_values[ind-1]) # appends the value of the pickup bin if it exists\n",
    "            else:\n",
    "                if i!=0:\n",
    "                    right_hand_limit=0\n",
    "                    for j in range(i,max_pickup_bins_len):\n",
    "                        if  j not in values[r-1]: #searches for the left-limit or the pickup-bin value which has a pickup value\n",
    "                            continue\n",
    "                        else:\n",
    "                            right_hand_limit=j\n",
    "                            break\n",
    "                    if right_hand_limit==0:\n",
    "                    #Case 1: When we have the last/last few values are found to be missing,hence we have no right-limit here\n",
    "                        smoothed_value=count_values[ind-1]*1.0/((max_pickup_bins_len-1-i)+2)*1.0                               \n",
    "                        for j in range(i,max_pickup_bins_len):                              \n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n",
    "                        repeat=(max_pickup_bins_len-1-i)\n",
    "                        ind-=1\n",
    "                    else:\n",
    "                    #Case 2: When we have the missing values between two known values\n",
    "                        smoothed_value=(count_values[ind-1]+count_values[ind])*1.0/((right_hand_limit-i)+2)*1.0             \n",
    "                        for j in range(i,right_hand_limit+1):\n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n",
    "                        repeat=(right_hand_limit-i)\n",
    "                else:\n",
    "                    #Case 3: When we have the first/first few values are found to be missing,hence we have no left-limit here\n",
    "                    right_hand_limit=0\n",
    "                    for j in range(i,max_pickup_bins_len):\n",
    "                        if  j not in values[r-1]:\n",
    "                            continue\n",
    "                        else:\n",
    "                            right_hand_limit=j\n",
    "                            break\n",
    "                    smoothed_value=count_values[ind]*1.0/((right_hand_limit-i)+1)*1.0\n",
    "                    for j in range(i,right_hand_limit+1):\n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                    repeat=(right_hand_limit-i)\n",
    "            ind+=1\n",
    "        smoothed_regions.extend(smoothed_bins)\n",
    "    return smoothed_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unq_pickup_bins(frame):\n",
    "    values = []\n",
    "    for i in nyc_region_number.values:\n",
    "    # for i in range(1,266):\n",
    "        new = frame[frame['PULocationID'] == i]\n",
    "        list_unq = list(set(new['pickup_bins']))\n",
    "        list_unq.sort()\n",
    "        values.append(list_unq)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_unique = []\n",
    "for frame in months_frame:\n",
    "    months_unique.append(return_unq_pickup_bins(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "13104\n",
      "69\n",
      "13104\n"
     ]
    }
   ],
   "source": [
    "months_smooth = []\n",
    "months_smooth_tip = []\n",
    "for groupby, unique in zip(months_groupby, months_unique):\n",
    "    # smoothing을 할 것인가 filling을 할것인가\n",
    "    months_smooth.append(smoothing(groupby['trip_distance'].values,unique))\n",
    "    months_smooth_tip.append(smoothing(groupby['tip_amount'].values,unique))\n",
    "    \n",
    "# Making list of all the values of pickup data in every bin for a period of 3 months and storing them region-wise \n",
    "regions_cum = []\n",
    "regions_cum_tip = []\n",
    "\n",
    "# number of 10min indices for jan 2019= 24*31*60/10 = 4464      # pickup_bins_len[0]\n",
    "# number of 10min indices for jan 2020 = 24*31*60/10 = 4464     # pickup_bins_len[0]\n",
    "# number of 10min indices for feb 2020 = 24*29*60/10 = 4176     # pickup_bins_len[1]\n",
    "# number of 10min indices for march 2020 = 24*31*60/10 = 4464   # pickup_bins_len[2]\n",
    "# regions_cum: it will contain 40 lists, each list will contain 4464+4176+4464 values which represents the number of pickups \n",
    "# that are happened for three months in 2016 data\n",
    "\n",
    "# nyc_regions_cnt개의 맨허튼 지역\n",
    "for i in range(1,nyc_regions_cnt+1):\n",
    "    cum = []\n",
    "    cum_tip = []\n",
    "    for index, smooth in enumerate(months_smooth):\n",
    "        cum += smooth[pickup_bins_len[index]*(i-1):pickup_bins_len[index]*i]\n",
    "    for index, smooth in enumerate(months_smooth_tip):\n",
    "        cum_tip += smooth[pickup_bins_len[index]*(i-1):pickup_bins_len[index]*i]\n",
    "    \n",
    "    regions_cum.append(cum)\n",
    "    regions_cum_tip.append(cum_tip)\n",
    "\n",
    "print(len(regions_cum))\n",
    "print(len(regions_cum[0]))\n",
    "print(len(regions_cum_tip))\n",
    "print(len(regions_cum_tip[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing data to be split into train and test, The below prepares data in cumulative form which will be later split into test and train\n",
    "# number of 10min indices for jan 2019= 24*31*60/10 = 4464      # pickup_bins_len[0]\n",
    "# number of 10min indices for jan 2020 = 24*31*60/10 = 4464     # pickup_bins_len[0]\n",
    "# number of 10min indices for feb 2020 = 24*29*60/10 = 4176     # pickup_bins_len[1]\n",
    "# number of 10min indices for march 2020 = 24*31*60/10 = 4464   # pickup_bins_len[2]\n",
    "# regions_cum: it will contain 40 lists, each list will contain 4464+4176+4464 values which represents the number of pickups \n",
    "# that are happened for three months in 2016 data\n",
    "\n",
    "# print(len(regions_cum))\n",
    "# 265\n",
    "# print(len(regions_cum[0]))\n",
    "# 4368\n",
    "\n",
    "\n",
    "# we take number of pickups that are happened in last 5 intravels\n",
    "number_of_time_stamps = 5\n",
    "\n",
    "# output varaible\n",
    "# it is list of lists\n",
    "# it will contain number of pickups 4368 for each cluster\n",
    "# len(regions_cum[0]) == 4368\n",
    "output = []\n",
    "output_tip = []\n",
    "sum(pickup_bins_len[:base_month_count])\n",
    "# 우리 데이터\n",
    "# len(regions_cum[0]) - 5(:= # of colunms)\n",
    "# 4368 - 5 = 4363\n",
    "# 13104 - 5 = 13099\n",
    "\n",
    "# tsne_lat will contain 13104-5=13099 times lattitude of cluster center for every cluster\n",
    "# Ex: [[cent_lat 13099times],[cent_lat 13099times], [cent_lat 13099times].... 40 lists]\n",
    "# it is list of lists\n",
    "# tsne_lat = []\n",
    "\n",
    "# tsne_lon will contain 13104-5=13099 times logitude of cluster center for every cluster\n",
    "# Ex: [[cent_long 13099times],[cent_long 13099times], [cent_long 13099times].... 40 lists]\n",
    "# it is list of lists\n",
    "# tsne_lon = []\n",
    "\n",
    "# 우리는 lat, lon 대신에 목적지 ID (PULocationID: 출발지, DOLocationID: 도착지)를 사용할 것이다.\n",
    "tsne_PULocationID = []\n",
    "\n",
    "#tsne_Tip_amount = []\n",
    "# we will code each day \n",
    "# sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5, sat=6\n",
    "# for every cluster we will be adding 13099 values, each value represent to which day of the week that pickup bin belongs to\n",
    "# it is list of lists\n",
    "tsne_weekday = []\n",
    "\n",
    "# its an numbpy array, of shape (523960, 5)\n",
    "# each row corresponds to an entry in out data\n",
    "# for the first row we will have [f0,f1,f2,f3,f4] fi=number of pickups happened in i+1th 10min intravel(bin)\n",
    "# the second row will have [f1,f2,f3,f4,f5]\n",
    "# the third row will have [f2,f3,f4,f5,f6]\n",
    "# and so on...\n",
    "tsne_feature = []\n",
    "\n",
    "\n",
    "tsne_feature = [0]*number_of_time_stamps\n",
    "for i in range(1,nyc_regions_cnt+1):\n",
    "    # tsne_lat.append([kmeans.cluster_centers_[i][0]]*13099) # kmeans.cluster_centers_[i][0] := Coordinates of cluster centers. 클러스트 센터의 상관계수\n",
    "    # tsne_lon.append([kmeans.cluster_centers_[i][1]]*13099)\n",
    "\n",
    "    # tsne_PULocationID\n",
    "    tsne_PULocationID.append([i]*(len(regions_cum[0]) - 5))\n",
    "    \n",
    "    #tsne_Tip_amount.append([i]*(len(regions_cum[0]) - 5))\n",
    "\n",
    "    day_of_the_week_dict = {2015: 4, 2016: 5, 2017: 0, 2018:1, 2019:2, 2020:3, 2021:5, 2022:6}\n",
    "    # jan 1st 2016 is thursday, so we start our day from 4: \"(int(k/144))%7+4\"\n",
    "    # our prediction start from 5th 10min intravel since we need to have number of pickups that are happened in last 5 pickup bins\n",
    "    \n",
    "    # jan 1st 2020 is tue -> 3\n",
    "    tsne_weekday.append([int(((int(k/144))%7+day_of_the_week_dict[base_year+1])%7) for k in range(5,sum(pickup_bins_len[:base_month_count]))])\n",
    "\n",
    "    # jan 1st 2021 is fri -> 5\n",
    "    # tsne_weekday.append([int(((int(k/144))%7+5)%7) for k in range(5,sum(pickup_bins_len[:3]))])\n",
    "    # regions_cum is a list of lists [[x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], .. 40 lsits]\n",
    "    \n",
    "    # 우리 데이터 \n",
    "    # regions_cum [[x_1,x_2,...,x_{len(regions_cum[0]) - 5}],...265 lists] len(regions_cum[0]) - 5 = 4381\n",
    "    tsne_feature = np.vstack((tsne_feature, [regions_cum[i-1][r:r+number_of_time_stamps] for r in range(0,len(regions_cum[i-1])-number_of_time_stamps)]))\n",
    "\n",
    "    output.append(regions_cum[i-1][5:])\n",
    "    output_tip.append(regions_cum_tip[i-1][5:])\n",
    "tsne_feature = tsne_feature[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "903831\n",
      "903831\n",
      "903831\n",
      "903831\n",
      "903831\n",
      "903831\n"
     ]
    }
   ],
   "source": [
    "print(tsne_feature.shape[0])\n",
    "print(len(tsne_weekday)*len(tsne_weekday[0]))\n",
    "print(len(output)*len(output[0]))\n",
    "print(len(output_tip)*len(output_tip[0]))\n",
    "print(nyc_regions_cnt*(len(regions_cum[0])-5))\n",
    "print(len(tsne_PULocationID)*len(tsne_PULocationID[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions of exponential moving averages to be used as a feature in cumulative form\n",
    "\n",
    "# upto now we computed 8 features for every data point that starts from 50th min of the day\n",
    "# 1. cluster center lattitude\n",
    "# 2. cluster center longitude\n",
    "# 3. day of the week \n",
    "# 4. f_t_1: number of pickups that are happened previous t-1th 10min intravel\n",
    "# 5. f_t_2: number of pickups that are happened previous t-2th 10min intravel\n",
    "# 6. f_t_3: number of pickups that are happened previous t-3th 10min intravel\n",
    "# 7. f_t_4: number of pickups that are happened previous t-4th 10min intravel\n",
    "# 8. f_t_5: number of pickups that are happened previous t-5th 10min intravel\n",
    "\n",
    "# from the baseline models we said the exponential weighted moving avarage gives us the best error\n",
    "# we will try to add the same exponential weighted moving avarage at t as a feature to our data\n",
    "# exponential weighted moving avarage => p'(t) = alpha*p'(t-1) + (1-alpha)*P(t-1) \n",
    "alpha=0.3\n",
    "\n",
    "# it is a temporary array that store exponential weighted moving avarage for each 10min intravel, \n",
    "# for each cluster it will get reset\n",
    "# for every cluster it contains 13104 values\n",
    "predicted_values=[]\n",
    "predicted_values_tip=[]\n",
    "\n",
    "# it is similar like tsne_lat\n",
    "# it is list of lists\n",
    "# predict_list is a list of lists [[x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], .. 40 lsits]\n",
    "predict_list = []\n",
    "predict_list_tip = []\n",
    "tsne_flat_exp_avg = []\n",
    "for r in range(1,nyc_regions_cnt+1):\n",
    "    for i in range(0,len(regions_cum[0])):\n",
    "        if i==0:\n",
    "            predicted_value= regions_cum[r-1][0]\n",
    "            predicted_values.append(0)\n",
    "            predicted_value_tip= regions_cum_tip[r-1][0]\n",
    "            predicted_values_tip.append(0)\n",
    "            continue\n",
    "        predicted_values.append(predicted_value)\n",
    "        predicted_value =int((alpha*predicted_value) + (1-alpha)*(regions_cum[r-1][i]))\n",
    "        predicted_values_tip.append(predicted_value_tip)\n",
    "        predicted_value_tip =(alpha*predicted_value_tip) + (1-alpha)*(regions_cum_tip[r-1][i])\n",
    "    \n",
    "    predict_list.append(predicted_values[5:])\n",
    "    predicted_values=[]\n",
    "    predict_list_tip.append(predicted_values_tip[5:])\n",
    "    predicted_values_tip=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data : 9169\n",
      "size of test data : 3929\n"
     ]
    }
   ],
   "source": [
    "# train, test split : 70% 30% split\n",
    "# Before we start predictions using the tree based regression models we take 3 months of 2016 pickup data \n",
    "# and split it such that for every region we have 70% data in train and 30% in test,\n",
    "# ordered date-wise for every region\n",
    "\n",
    "sizeof_train_data = int((len(regions_cum[0])-5)*0.7)\n",
    "sizeof_test_data = int((len(regions_cum[0])-5)*0.3)\n",
    "\n",
    "\n",
    "print(\"size of train data :\", sizeof_train_data)\n",
    "print(\"size of test data :\", sizeof_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting first 91nyc_regions_cnt timestamp values i.e 70% of 13099 (total timestamps) for our training data\n",
    "train_features =  [tsne_feature[i*(len(regions_cum[0])-5):((len(regions_cum[0])-5)*i+sizeof_train_data)] for i in range(0,nyc_regions_cnt)]\n",
    "\n",
    "test_features = [tsne_feature[((len(regions_cum[0])-5)*(i))+sizeof_train_data:(len(regions_cum[0])-5)*(i+1)] for i in range(0,nyc_regions_cnt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data clusters 69 Number of data points in trian data 9169 Each data point contains 5 features\n",
      "Number of data clusters 69 Number of data points in test data 3930 Each data point contains 5 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data clusters\",len(train_features), \"Number of data points in trian data\", len(train_features[0]), \"Each data point contains\", len(train_features[0][0]),\"features\")\n",
    "print(\"Number of data clusters\",len(train_features), \"Number of data points in test data\", len(test_features[0]), \"Each data point contains\", len(test_features[0][0]),\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting first sizeof_train_data timestamp values i.e 70% of 13099 (total timestamps) for our training data\n",
    "\n",
    "tsne_train_flat_PULocationID = [i[:sizeof_train_data] for i in tsne_PULocationID]\n",
    "tsne_train_flat_Tip_amount = [i[:sizeof_train_data] for i in predict_list_tip]\n",
    "tsne_train_flat_weekday = [i[:sizeof_train_data] for i in tsne_weekday]\n",
    "tsne_train_flat_output = [i[:sizeof_train_data] for i in output]\n",
    "tsne_train_flat_output_tip = [i[:sizeof_train_data] for i in output_tip]\n",
    "tsne_train_flat_exp_avg = [i[:sizeof_train_data] for i in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the rest of the timestamp values i.e 30% of sizeof_train_data + sizeof_test_data (total timestamps) for our test data\n",
    "\n",
    "tsne_test_flat_PULocationID = [i[sizeof_train_data:] for i in tsne_PULocationID]\n",
    "tsne_test_flat_Tip_amount = [i[sizeof_train_data:] for i in predict_list_tip]\n",
    "tsne_test_flat_weekday = [i[sizeof_train_data:] for i in tsne_weekday]\n",
    "tsne_test_flat_output = [i[sizeof_train_data:] for i in output]\n",
    "tsne_test_flat_output_tip = [i[sizeof_train_data:] for i in output_tip]\n",
    "tsne_test_flat_exp_avg = [i[sizeof_train_data:] for i in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above contains values in the form of list of lists (i.e. list of values of each region), here we make all of them in one list\n",
    "train_new_features = []\n",
    "for i in range(0,nyc_regions_cnt):\n",
    "    train_new_features.extend(train_features[i])\n",
    "test_new_features = []\n",
    "for i in range(0,nyc_regions_cnt):\n",
    "    test_new_features.extend(test_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_train_PULocationID = sum(tsne_train_flat_PULocationID, [])\n",
    "tsne_train_Tip_amount = sum(tsne_train_flat_Tip_amount, [])\n",
    "tsne_train_weekday = sum(tsne_train_flat_weekday, [])\n",
    "tsne_train_output = sum(tsne_train_flat_output, [])\n",
    "tsne_train_output_tip = sum(tsne_train_flat_output_tip, [])\n",
    "tsne_train_exp_avg = sum(tsne_train_flat_exp_avg,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_test_PULocationID = sum(tsne_test_flat_PULocationID, [])\n",
    "tsne_test_Tip_amount = sum(tsne_test_flat_Tip_amount, [])\n",
    "tsne_test_weekday = sum(tsne_test_flat_weekday, [])\n",
    "tsne_test_output = sum(tsne_test_flat_output, [])\n",
    "tsne_test_output_tip = sum(tsne_test_flat_output_tip, [])\n",
    "tsne_test_exp_avg = sum(tsne_test_flat_exp_avg,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(632661, 9)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data frame for our train data\n",
    "columns = ['ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "df_train = pd.DataFrame(data=train_new_features, columns=columns) \n",
    "# df_train['lat'] = tsne_train_lat\n",
    "# df_train['lon'] = tsne_train_lon\n",
    "\n",
    "df_train['PULocationID'] = tsne_train_PULocationID\n",
    "df_train['Tip_amount'] = tsne_train_Tip_amount\n",
    "df_train['weekday'] = tsne_train_weekday\n",
    "df_train['exp_avg'] = tsne_train_exp_avg\n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(271170, 9)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data frame for our train data\n",
    "df_test = pd.DataFrame(data=test_new_features, columns=columns) \n",
    "# df_test['lat'] = tsne_test_lat\n",
    "# df_test['lon'] = tsne_test_lon\n",
    "\n",
    "df_test['PULocationID'] = tsne_test_PULocationID\n",
    "df_test['Tip_amount'] = tsne_test_Tip_amount\n",
    "df_test['weekday'] = tsne_test_weekday\n",
    "df_test['exp_avg'] = tsne_test_exp_avg\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find more about LinearRegression function here http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
    "\n",
    "# some of methods of LinearRegression()\n",
    "# fit(X, y[, sample_weight])\tFit linear model.\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict using the linear model\n",
    "# score(X, y[, sample_weight])\tReturns the coefficient of determination R^2 of the prediction.\n",
    "# set_params(**params)\tSet the parameters of this estimator.\n",
    "# -----------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1-2-copy-8/\n",
    "# -----------------------\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_reg=LinearRegression().fit(df_train, tsne_train_output)\n",
    "\n",
    "y_pred = lr_reg.predict(df_test)\n",
    "lr_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = lr_reg.predict(df_train)\n",
    "lr_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3911790570314858\n",
      "0.473940192412189\n"
     ]
    }
   ],
   "source": [
    "lr_reg_tip=LinearRegression().fit(df_train, tsne_train_output_tip)\n",
    "y_pred_test_tip = lr_reg_tip.predict(df_test)\n",
    "y_pred_train_tip = lr_reg_tip.predict(df_train)\n",
    "print(r2_score(tsne_train_output_tip,y_pred_train_tip))\n",
    "print(r2_score(tsne_test_output_tip,y_pred_test_tip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_features=&#x27;sqrt&#x27;, min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_features=&#x27;sqrt&#x27;, min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_features='sqrt', min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned random forest regressor on our train data\n",
    "# find more about LinearRegression function here http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion=’mse’, max_depth=None, min_samples_split=2, \n",
    "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# apply(X)\tApply trees in the forest to X, return leaf indices.\n",
    "# decision_path(X)\tReturn the decision path in the forest\n",
    "# fit(X, y[, sample_weight])\tBuild a forest of trees from the training set (X, y).\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict regression target for X.\n",
    "# score(X, y[, sample_weight])\tReturns the coefficient of determination R^2 of the prediction.\n",
    "# -----------------------\n",
    "# video link1: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/regression-using-decision-trees-2/\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "regr1 = RandomForestRegressor(max_features='sqrt',min_samples_leaf=4,min_samples_split=3,n_estimators=40, n_jobs=-1)\n",
    "regr1.fit(df_train, tsne_train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data using our trained random forest model \n",
    "\n",
    "# the models regr1 is already hyper parameter tuned\n",
    "# the parameters that we got above are found using grid search\n",
    "\n",
    "y_pred = regr1.predict(df_test)\n",
    "rndf_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = regr1.predict(df_train)\n",
    "rndf_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ft_5', 'ft_4', 'ft_3', 'ft_2', 'ft_1', 'PULocationID', 'Tip_amount',\n",
      "       'weekday', 'exp_avg'],\n",
      "      dtype='object')\n",
      "[0.03877159 0.04509688 0.13898443 0.18379366 0.28636623 0.00635238\n",
      " 0.00985344 0.003211   0.28757038]\n"
     ]
    }
   ],
   "source": [
    "#feature importances based on analysis using random forest\n",
    "print (df_train.columns)\n",
    "print (regr1.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using XgBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "             random_state=0, reg_alpha=200, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "             random_state=0, reg_alpha=200, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor='auto',\n",
       "             random_state=0, reg_alpha=200, ...)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned Xg-Boost regressor on our train data\n",
    "\n",
    "# find more about XGBRegressor function here http://xgboost.readthedocs.io/en/latest/python/python_api.html?#module-xgboost.sklearn\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# xgboost.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', \n",
    "# booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, \n",
    "# colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, \n",
    "# missing=None, **kwargs)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(data, output_margin=False, ntree_limit=0) : Predict with data. NOTE: This function is not thread safe.\n",
    "# get_score(importance_type='weight') -> get the feature importance\n",
    "# -----------------------\n",
    "# video link1: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/regression-using-decision-trees-2/\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "x_model = xgb.XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=3,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " reg_alpha=200, reg_lambda=200,\n",
    " colsample_bytree=0.8,nthread=4)\n",
    "x_model.fit(df_train, tsne_train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting with our trained Xg-Boost regressor\n",
    "# the models x_model is already hyper parameter tuned\n",
    "# the parameters that we got above are found using grid search\n",
    "\n",
    "y_pred = x_model.predict(df_test)\n",
    "xgb_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = x_model.predict(df_train)\n",
    "xgb_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ft_5': 788.0,\n",
       " 'ft_4': 592.0,\n",
       " 'ft_3': 762.0,\n",
       " 'ft_2': 902.0,\n",
       " 'ft_1': 1043.0,\n",
       " 'PULocationID': 883.0,\n",
       " 'Tip_amount': 1000.0,\n",
       " 'weekday': 161.0,\n",
       " 'exp_avg': 791.0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature importances\n",
    "x_model.get_booster().get_score(importance_type=\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the error metric values for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_R2=[]\n",
    "test_R2=[]\n",
    "\n",
    "train_R2.append(r2_score(tsne_train_output,df_train['ft_1'].values))\n",
    "train_R2.append(r2_score(tsne_train_output,df_train['exp_avg'].values))\n",
    "train_R2.append(r2_score(tsne_train_output,rndf_train_predictions))\n",
    "train_R2.append(r2_score(tsne_train_output,xgb_train_predictions))\n",
    "train_R2.append(r2_score(tsne_train_output,lr_train_predictions))\n",
    "train_R2.append(r2_score(tsne_train_output_tip,df_train['Tip_amount'].values))\n",
    "\n",
    "test_R2.append(r2_score(tsne_test_output,df_test['ft_1'].values))\n",
    "test_R2.append(r2_score(tsne_test_output,df_test['exp_avg'].values))\n",
    "test_R2.append(r2_score(tsne_test_output,rndf_test_predictions))\n",
    "test_R2.append(r2_score(tsne_test_output,xgb_test_predictions))\n",
    "test_R2.append(r2_score(tsne_test_output,lr_test_predictions))\n",
    "test_R2.append(r2_score(tsne_test_output_tip,df_test['Tip_amount'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 1 ~ 3 Manhattan R2\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Baseline Model\t\t\t\tTrain: \t0.9039618657396649\tTest: \t0.9160986612230623\n",
      "Exponential Averages Forecasting\tTrain: \t0.915085028941301\tTest: \t0.9268716502879364\n",
      "Random Forest Regression\t\tTrain: \t0.959487398256358\tTest: \t0.9313757654335548\n",
      "XgBoost Regression\t\t\tTrain: \t0.9241338584239618\tTest: \t0.9325510062866035\n",
      "Linear Regression\t\t\tTrain: \t0.9175430327589593\tTest: \t0.929399648144614\n",
      "Baseline Model of Tip\t\t\tTrain: \t0.2996845826892446\tTest: \t0.41367387208454387\n"
     ]
    }
   ],
   "source": [
    "print(f\"{base_year+1} 1 ~ {base_month_count} {region} R2\")\n",
    "print (\"--------------------------------------------------------------------------------------------------------\")\n",
    "print (\"Baseline Model\\t\\t\\t\",\"Train: \",train_R2[0],\"Test: \",test_R2[0],sep='\\t')\n",
    "print (\"Exponential Averages Forecasting\",\"Train: \",train_R2[1],\"Test: \",test_R2[1],sep='\\t')\n",
    "print (\"Random Forest Regression\\t\",\"Train: \",train_R2[2],\"Test: \",test_R2[2],sep='\\t')\n",
    "print (\"XgBoost Regression\\t\\t\",\"Train: \",train_R2[3],\"Test: \",test_R2[3],sep='\\t')\n",
    "print (\"Linear Regression\\t\\t\",\"Train: \",train_R2[4],\"Test: \",test_R2[4],sep='\\t')\n",
    "print (\"Baseline Model of Tip\\t\\t\",\"Train: \",train_R2[5],\"Test: \",test_R2[5],sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MAE=[]\n",
    "test_MAE=[]\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,df_train['ft_1'].values))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,df_train['exp_avg'].values))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,rndf_train_predictions))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,xgb_train_predictions))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,lr_train_predictions))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output_tip,df_train['Tip_amount'].values))\n",
    "\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,df_test['ft_1'].values))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,df_test['exp_avg'].values))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,rndf_test_predictions))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,xgb_test_predictions))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,lr_test_predictions))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output_tip,df_test['Tip_amount'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 1 ~ 3 Manhattan MAE\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Baseline Model\t\t\t\tTrain: \t3.1469649622783766\tTest: \t1.483128664675296\n",
      "Exponential Averages Forecasting\tTrain: \t2.966514452447677\tTest: \t1.4045432754360734\n",
      "Random Forest Regression\t\tTrain: \t2.048171137465404\tTest: \t1.4130545414315743\n",
      "XgBoost Regression\t\t\tTrain: \t2.8720736697852405\tTest: \t1.3794667551720323\n",
      "Linear Regression\t\t\tTrain: \t2.945013206124607\tTest: \t1.400184386178412\n",
      "Baseline Model of Tip\t\t\tTrain: \t0.4700618260227975\tTest: \t0.36395292465735557\n"
     ]
    }
   ],
   "source": [
    "print(f\"{base_year+1} 1 ~ {base_month_count} {region} MAE\")\n",
    "print (\"--------------------------------------------------------------------------------------------------------\")\n",
    "print (\"Baseline Model\\t\\t\\t\",\"Train: \",train_MAE[0],\"Test: \",test_MAE[0],sep='\\t')\n",
    "print (\"Exponential Averages Forecasting\",\"Train: \",train_MAE[1],\"Test: \",test_MAE[1],sep='\\t')\n",
    "print (\"Random Forest Regression\\t\",\"Train: \",train_MAE[2],\"Test: \",test_MAE[2],sep='\\t')\n",
    "print (\"XgBoost Regression\\t\\t\",\"Train: \",train_MAE[3],\"Test: \",test_MAE[3],sep='\\t')\n",
    "print (\"Linear Regression\\t\\t\",\"Train: \",train_MAE[4],\"Test: \",test_MAE[4],sep='\\t')\n",
    "print (\"Baseline Model of Tip\\t\\t\",\"Train: \",train_MAE[5],\"Test: \",test_MAE[5],sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
