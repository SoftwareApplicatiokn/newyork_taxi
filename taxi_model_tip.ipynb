{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unix time: https://www.unixtimestamp.com/\n",
    "import datetime  # Convert to unix time\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time  # Convert to unix time\n",
    "import warnings\n",
    "\n",
    "import dask.dataframe as dd  # similar to pandas\n",
    "import matplotlib.pylab as plt\n",
    "# if numpy is not installed already : pip3 install numpy\n",
    "import numpy as np  # Do aritmetic operations on arrays\n",
    "import pandas as pd  # pandas to create small dataframes\n",
    "import seaborn as sns  # Plots\n",
    "# to install xgboost: pip3 install xgboost\n",
    "# if it didnt happen check install_xgboost.JPG\n",
    "import xgboost as xgb\n",
    "from matplotlib import rcParams  # Size of plots\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans  # Clustering\n",
    "# to install sklearn: pip install -U scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (mean_absolute_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_squared_error, r2_score)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_year = 2018\n",
    "base_month_count = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    months_frame = []\n",
    "    months_groupby = []\n",
    "    for i in range(1,base_month_count+1):\n",
    "        tmp_frame = pd.read_parquet(f'preprocessing_yellow_tripdata_{base_year+1}_{i}.parquet',engine='pyarrow')\n",
    "        \n",
    "        tmp_groupby = tmp_frame[['PULocationID','pickup_bins','trip_distance']].groupby(['PULocationID','pickup_bins'], dropna=False).count()\n",
    "        tmp_tip_amount = tmp_frame[['PULocationID','pickup_bins','tip_amount']].groupby(['PULocationID','pickup_bins'], dropna=False).mean()\n",
    "        #print(tmp_groupby)\n",
    "        # print(\"-------------------------\")\n",
    "        #print(tmp_tip_amount)\n",
    "        # print(\"-------------------------\")\n",
    "        #tmp_groupby = pd.concat([tmp_trip_distance, tmp_tip_amount])\n",
    "        #print(tmp_groupby)\n",
    "        tmp_groupby = pd.merge(tmp_groupby, tmp_tip_amount, on = ['PULocationID','pickup_bins'], how = \"left\")\n",
    "\n",
    "        \n",
    "        print(tmp_groupby)\n",
    "        months_frame.append(tmp_frame)\n",
    "        months_groupby.append(tmp_groupby)\n",
    "    return months_frame, months_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          trip_distance  tip_amount\n",
      "PULocationID pickup_bins                           \n",
      "4            -52                      1    0.000000\n",
      "              0                       1    0.000000\n",
      "              1                       4    1.450000\n",
      "              2                      11    2.130909\n",
      "              3                       9    1.034444\n",
      "...                                 ...         ...\n",
      "263           4459                   17    1.778824\n",
      "              4460                   25    0.873200\n",
      "              4461                   23    1.158261\n",
      "              4462                   17    1.254706\n",
      "              4463                   24    1.257917\n",
      "\n",
      "[249105 rows x 2 columns]\n",
      "                          trip_distance  tip_amount\n",
      "PULocationID pickup_bins                           \n",
      "4            0                        4    2.380000\n",
      "             1                        7    1.081429\n",
      "             2                        2    0.000000\n",
      "             3                        2    2.500000\n",
      "             4                        4    1.352500\n",
      "...                                 ...         ...\n",
      "263          14109                    1    2.000000\n",
      "             14615                    1    4.000000\n",
      "             17688                    1    1.560000\n",
      "             19719                    1    2.560000\n",
      "             1001787                  1    0.000000\n",
      "\n",
      "[225795 rows x 2 columns]\n",
      "                          trip_distance  tip_amount\n",
      "PULocationID pickup_bins                           \n",
      "4            0                        8    1.158750\n",
      "             1                        8    2.356250\n",
      "             2                        3    0.500000\n",
      "             3                        4    2.267500\n",
      "             4                        4    1.715000\n",
      "...                                 ...         ...\n",
      "263          4461                     9    1.461111\n",
      "             4462                     8    0.902500\n",
      "             4463                     4    1.405000\n",
      "             6044                     1    1.660000\n",
      "             6048                     1    1.760000\n",
      "\n",
      "[250844 rows x 2 columns]\n",
      "                          trip_distance  tip_amount\n",
      "PULocationID pickup_bins                           \n",
      "4            0                        1    0.000000\n",
      "             1                        4    2.942500\n",
      "             2                        1    1.650000\n",
      "             3                        2    0.850000\n",
      "             5                        3    0.850000\n",
      "...                                 ...         ...\n",
      "263          4317                    23    2.469565\n",
      "             4318                    13    1.743846\n",
      "             4319                    16    1.981875\n",
      "             4320                     1    2.360000\n",
      "             740238                   1    0.000000\n",
      "\n",
      "[242167 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "months_frame, months_groupby = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴욕 지역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_df = pd.read_csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"Manhattan\"\n",
    "nyc_region = taxi_zone_df[taxi_zone_df['Borough'] == region]\n",
    "nyc_region_number = nyc_region['LocationID']\n",
    "nyc_regions_cnt = len(nyc_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4464\n"
     ]
    }
   ],
   "source": [
    "# number of 10min indices for jan 2019= 24*31*60/10 = max_pickup_bins_len\n",
    "interval = 10\n",
    "days = [31,29,31,30,31,30,31,31,30,31,30,31]\n",
    "pickup_bins_len = []\n",
    "\n",
    "for day in days:\n",
    "    pickup_bins_len.append(int(24*60*day/interval))\n",
    "max_pickup_bins_len = max(pickup_bins_len)\n",
    "print(max_pickup_bins_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills a value of zero for every bin where no pickup data is present \n",
    "# the count_values: number pickps that are happened in each region for each 10min intravel\n",
    "# there wont be any value if there are no picksups.\n",
    "# values: number of unique bins\n",
    "\n",
    "# for every 10min intravel(pickup_bin) we will check it is there in our unique bin,\n",
    "# if it is there we will add the count_values[index] to smoothed data\n",
    "# if not we add smoothed data (which is calculated based on the methods that are discussed in the above markdown cell)\n",
    "# we finally return smoothed data\n",
    "def smoothing(count_values,values):\n",
    "    smoothed_regions=[] # stores list of final smoothed values of each reigion\n",
    "    ind=0\n",
    "    repeat=0 \n",
    "    smoothed_value=0\n",
    "    for r in range(1,nyc_regions_cnt+1):\n",
    "        smoothed_bins=[] #stores the final smoothed values\n",
    "        repeat=0\n",
    "        for i in range(max_pickup_bins_len):\n",
    "            if repeat!=0: # prevents iteration for a value which is already visited/resolved\n",
    "                repeat-=1\n",
    "                continue\n",
    "            if i in values[r-1]: #checks if the pickup-bin exists \n",
    "                smoothed_bins.append(count_values[ind-1]) # appends the value of the pickup bin if it exists\n",
    "            else:\n",
    "                if i!=0:\n",
    "                    right_hand_limit=0\n",
    "                    for j in range(i,max_pickup_bins_len):\n",
    "                        if  j not in values[r-1]: #searches for the left-limit or the pickup-bin value which has a pickup value\n",
    "                            continue\n",
    "                        else:\n",
    "                            right_hand_limit=j\n",
    "                            break\n",
    "                    if right_hand_limit==0:\n",
    "                    #Case 1: When we have the last/last few values are found to be missing,hence we have no right-limit here\n",
    "                        smoothed_value=count_values[ind-1]*1.0/((max_pickup_bins_len-1-i)+2)*1.0                               \n",
    "                        for j in range(i,max_pickup_bins_len):                              \n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n",
    "                        repeat=(max_pickup_bins_len-1-i)\n",
    "                        ind-=1\n",
    "                    else:\n",
    "                    #Case 2: When we have the missing values between two known values\n",
    "                        smoothed_value=(count_values[ind-1]+count_values[ind])*1.0/((right_hand_limit-i)+2)*1.0             \n",
    "                        for j in range(i,right_hand_limit+1):\n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                        smoothed_bins[i-1] = math.ceil(smoothed_value)\n",
    "                        repeat=(right_hand_limit-i)\n",
    "                else:\n",
    "                    #Case 3: When we have the first/first few values are found to be missing,hence we have no left-limit here\n",
    "                    right_hand_limit=0\n",
    "                    for j in range(i,max_pickup_bins_len):\n",
    "                        if  j not in values[r-1]:\n",
    "                            continue\n",
    "                        else:\n",
    "                            right_hand_limit=j\n",
    "                            break\n",
    "                    smoothed_value=count_values[ind]*1.0/((right_hand_limit-i)+1)*1.0\n",
    "                    for j in range(i,right_hand_limit+1):\n",
    "                            smoothed_bins.append(math.ceil(smoothed_value))\n",
    "                    repeat=(right_hand_limit-i)\n",
    "            ind+=1\n",
    "        smoothed_regions.extend(smoothed_bins)\n",
    "    return smoothed_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unq_pickup_bins(frame):\n",
    "    values = []\n",
    "    for i in nyc_region_number.values:\n",
    "    # for i in range(1,266):\n",
    "        new = frame[frame['PULocationID'] == i]\n",
    "        list_unq = list(set(new['pickup_bins']))\n",
    "        list_unq.sort()\n",
    "        values.append(list_unq)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_unique = []\n",
    "for frame in months_frame:\n",
    "    months_unique.append(return_unq_pickup_bins(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "17424\n",
      "69\n",
      "17424\n"
     ]
    }
   ],
   "source": [
    "months_smooth = []\n",
    "months_smooth_tip = []\n",
    "for groupby, unique in zip(months_groupby, months_unique):\n",
    "    # smoothing을 할 것인가 filling을 할것인가\n",
    "    months_smooth.append(smoothing(groupby['trip_distance'].values,unique))\n",
    "    months_smooth_tip.append(smoothing(groupby['tip_amount'].values,unique))\n",
    "    \n",
    "# Making list of all the values of pickup data in every bin for a period of 3 months and storing them region-wise \n",
    "regions_cum = []\n",
    "regions_cum_tip = []\n",
    "\n",
    "# number of 10min indices for jan 2019= 24*31*60/10 = 4464      # pickup_bins_len[0]\n",
    "# number of 10min indices for jan 2020 = 24*31*60/10 = 4464     # pickup_bins_len[0]\n",
    "# number of 10min indices for feb 2020 = 24*29*60/10 = 4176     # pickup_bins_len[1]\n",
    "# number of 10min indices for march 2020 = 24*31*60/10 = 4464   # pickup_bins_len[2]\n",
    "# regions_cum: it will contain 40 lists, each list will contain 4464+4176+4464 values which represents the number of pickups \n",
    "# that are happened for three months in 2016 data\n",
    "\n",
    "# nyc_regions_cnt개의 맨허튼 지역\n",
    "for i in range(1,nyc_regions_cnt+1):\n",
    "    cum = []\n",
    "    cum_tip = []\n",
    "    for index, smooth in enumerate(months_smooth):\n",
    "        cum += smooth[pickup_bins_len[index]*(i-1):pickup_bins_len[index]*i]\n",
    "    for index, smooth in enumerate(months_smooth_tip):\n",
    "        cum_tip += smooth[pickup_bins_len[index]*(i-1):pickup_bins_len[index]*i]\n",
    "    \n",
    "    regions_cum.append(cum)\n",
    "    regions_cum_tip.append(cum_tip)\n",
    "\n",
    "print(len(regions_cum))\n",
    "print(len(regions_cum[0]))\n",
    "print(len(regions_cum_tip))\n",
    "print(len(regions_cum_tip[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing data to be split into train and test, The below prepares data in cumulative form which will be later split into test and train\n",
    "# number of 10min indices for jan 2019= 24*31*60/10 = 4464      # pickup_bins_len[0]\n",
    "# number of 10min indices for jan 2020 = 24*31*60/10 = 4464     # pickup_bins_len[0]\n",
    "# number of 10min indices for feb 2020 = 24*29*60/10 = 4176     # pickup_bins_len[1]\n",
    "# number of 10min indices for march 2020 = 24*31*60/10 = 4464   # pickup_bins_len[2]\n",
    "# regions_cum: it will contain 40 lists, each list will contain 4464+4176+4464 values which represents the number of pickups \n",
    "# that are happened for three months in 2016 data\n",
    "\n",
    "# print(len(regions_cum))\n",
    "# 265\n",
    "# print(len(regions_cum[0]))\n",
    "# 4368\n",
    "\n",
    "\n",
    "# we take number of pickups that are happened in last 5 intravels\n",
    "number_of_time_stamps = 5\n",
    "\n",
    "# output varaible\n",
    "# it is list of lists\n",
    "# it will contain number of pickups 4368 for each cluster\n",
    "# len(regions_cum[0]) == 4368\n",
    "output = []\n",
    "output_tip = []\n",
    "sum(pickup_bins_len[:base_month_count])\n",
    "# 우리 데이터\n",
    "# len(regions_cum[0]) - 5(:= # of colunms)\n",
    "# 4368 - 5 = 4363\n",
    "# 13104 - 5 = 13099\n",
    "\n",
    "# tsne_lat will contain 13104-5=13099 times lattitude of cluster center for every cluster\n",
    "# Ex: [[cent_lat 13099times],[cent_lat 13099times], [cent_lat 13099times].... 40 lists]\n",
    "# it is list of lists\n",
    "# tsne_lat = []\n",
    "\n",
    "# tsne_lon will contain 13104-5=13099 times logitude of cluster center for every cluster\n",
    "# Ex: [[cent_long 13099times],[cent_long 13099times], [cent_long 13099times].... 40 lists]\n",
    "# it is list of lists\n",
    "# tsne_lon = []\n",
    "\n",
    "# 우리는 lat, lon 대신에 목적지 ID (PULocationID: 출발지, DOLocationID: 도착지)를 사용할 것이다.\n",
    "tsne_PULocationID = []\n",
    "\n",
    "#tsne_Tip_amount = []\n",
    "# we will code each day \n",
    "# sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5, sat=6\n",
    "# for every cluster we will be adding 13099 values, each value represent to which day of the week that pickup bin belongs to\n",
    "# it is list of lists\n",
    "tsne_weekday = []\n",
    "\n",
    "# its an numbpy array, of shape (523960, 5)\n",
    "# each row corresponds to an entry in out data\n",
    "# for the first row we will have [f0,f1,f2,f3,f4] fi=number of pickups happened in i+1th 10min intravel(bin)\n",
    "# the second row will have [f1,f2,f3,f4,f5]\n",
    "# the third row will have [f2,f3,f4,f5,f6]\n",
    "# and so on...\n",
    "tsne_feature = []\n",
    "\n",
    "\n",
    "tsne_feature = [0]*number_of_time_stamps\n",
    "for i in range(1,nyc_regions_cnt+1):\n",
    "    # tsne_lat.append([kmeans.cluster_centers_[i][0]]*13099) # kmeans.cluster_centers_[i][0] := Coordinates of cluster centers. 클러스트 센터의 상관계수\n",
    "    # tsne_lon.append([kmeans.cluster_centers_[i][1]]*13099)\n",
    "\n",
    "    # tsne_PULocationID\n",
    "    tsne_PULocationID.append([i]*(len(regions_cum[0]) - 5))\n",
    "    \n",
    "    #tsne_Tip_amount.append([i]*(len(regions_cum[0]) - 5))\n",
    "\n",
    "    day_of_the_week_dict = {2015: 4, 2016: 5, 2017: 1, 2018:1, 2019:2, 2020:3, 2021:5, 2022:6}\n",
    "    # jan 1st 2016 is thursday, so we start our day from 4: \"(int(k/144))%7+4\"\n",
    "    # our prediction start from 5th 10min intravel since we need to have number of pickups that are happened in last 5 pickup bins\n",
    "    \n",
    "    # jan 1st 2020 is tue -> 3\n",
    "    tsne_weekday.append([int(((int(k/144))%7+day_of_the_week_dict[base_year+1])%7) for k in range(5,sum(pickup_bins_len[:base_month_count]))])\n",
    "\n",
    "    # jan 1st 2021 is fri -> 5\n",
    "    # tsne_weekday.append([int(((int(k/144))%7+5)%7) for k in range(5,sum(pickup_bins_len[:3]))])\n",
    "    # regions_cum is a list of lists [[x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], [x1,x2,x3..x13104], .. 40 lsits]\n",
    "    \n",
    "    # 우리 데이터 \n",
    "    # regions_cum [[x_1,x_2,...,x_{len(regions_cum[0]) - 5}],...265 lists] len(regions_cum[0]) - 5 = 4381\n",
    "    tsne_feature = np.vstack((tsne_feature, [regions_cum[i-1][r:r+number_of_time_stamps] for r in range(0,len(regions_cum[i-1])-number_of_time_stamps)]))\n",
    "\n",
    "    output.append(regions_cum[i-1][5:])\n",
    "    output_tip.append(regions_cum_tip[i-1][5:])\n",
    "tsne_feature = tsne_feature[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201911\n",
      "1201911\n",
      "1201911\n",
      "1201911\n",
      "1201911\n",
      "1201911\n"
     ]
    }
   ],
   "source": [
    "print(tsne_feature.shape[0])\n",
    "print(len(tsne_weekday)*len(tsne_weekday[0]))\n",
    "print(len(output)*len(output[0]))\n",
    "print(len(output_tip)*len(output_tip[0]))\n",
    "print(nyc_regions_cnt*(len(regions_cum[0])-5))\n",
    "print(len(tsne_PULocationID)*len(tsne_PULocationID[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predictions of exponential moving averages to be used as a feature in cumulative form\n",
    "\n",
    "# upto now we computed 8 features for every data point that starts from 50th min of the day\n",
    "# 1. cluster center lattitude\n",
    "# 2. cluster center longitude\n",
    "# 3. day of the week \n",
    "# 4. f_t_1: number of pickups that are happened previous t-1th 10min intravel\n",
    "# 5. f_t_2: number of pickups that are happened previous t-2th 10min intravel\n",
    "# 6. f_t_3: number of pickups that are happened previous t-3th 10min intravel\n",
    "# 7. f_t_4: number of pickups that are happened previous t-4th 10min intravel\n",
    "# 8. f_t_5: number of pickups that are happened previous t-5th 10min intravel\n",
    "\n",
    "# from the baseline models we said the exponential weighted moving avarage gives us the best error\n",
    "# we will try to add the same exponential weighted moving avarage at t as a feature to our data\n",
    "# exponential weighted moving avarage => p'(t) = alpha*p'(t-1) + (1-alpha)*P(t-1) \n",
    "alpha=0.3\n",
    "\n",
    "# it is a temporary array that store exponential weighted moving avarage for each 10min intravel, \n",
    "# for each cluster it will get reset\n",
    "# for every cluster it contains 13104 values\n",
    "predicted_values=[]\n",
    "predicted_values_tip=[]\n",
    "\n",
    "# it is similar like tsne_lat\n",
    "# it is list of lists\n",
    "# predict_list is a list of lists [[x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], [x5,x6,x7..x13104], .. 40 lsits]\n",
    "predict_list = []\n",
    "predict_list_tip = []\n",
    "tsne_flat_exp_avg = []\n",
    "for r in range(1,nyc_regions_cnt+1):\n",
    "    for i in range(0,len(regions_cum[0])):\n",
    "        if i==0:\n",
    "            predicted_value= regions_cum[r-1][0]\n",
    "            predicted_values.append(0)\n",
    "            predicted_value_tip= regions_cum_tip[r-1][0]\n",
    "            predicted_values_tip.append(0)\n",
    "            continue\n",
    "        predicted_values.append(predicted_value)\n",
    "        predicted_value =int((alpha*predicted_value) + (1-alpha)*(regions_cum[r-1][i]))\n",
    "        predicted_values_tip.append(predicted_value_tip)\n",
    "        predicted_value_tip =(alpha*predicted_value_tip) + (1-alpha)*(regions_cum_tip[r-1][i])\n",
    "    \n",
    "    predict_list.append(predicted_values[5:])\n",
    "    predicted_values=[]\n",
    "    predict_list_tip.append(predicted_values_tip[5:])\n",
    "    predicted_values_tip=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train data : 12193\n",
      "size of test data : 5225\n"
     ]
    }
   ],
   "source": [
    "# train, test split : 70% 30% split\n",
    "# Before we start predictions using the tree based regression models we take 3 months of 2016 pickup data \n",
    "# and split it such that for every region we have 70% data in train and 30% in test,\n",
    "# ordered date-wise for every region\n",
    "\n",
    "sizeof_train_data = int((len(regions_cum[0])-5)*0.7)\n",
    "sizeof_test_data = int((len(regions_cum[0])-5)*0.3)\n",
    "\n",
    "\n",
    "print(\"size of train data :\", sizeof_train_data)\n",
    "print(\"size of test data :\", sizeof_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting first 91nyc_regions_cnt timestamp values i.e 70% of 13099 (total timestamps) for our training data\n",
    "train_features =  [tsne_feature[i*(len(regions_cum[0])-5):((len(regions_cum[0])-5)*i+sizeof_train_data)] for i in range(0,nyc_regions_cnt)]\n",
    "\n",
    "test_features = [tsne_feature[((len(regions_cum[0])-5)*(i))+sizeof_train_data:(len(regions_cum[0])-5)*(i+1)] for i in range(0,nyc_regions_cnt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data clusters 69 Number of data points in trian data 12193 Each data point contains 5 features\n",
      "Number of data clusters 69 Number of data points in test data 5226 Each data point contains 5 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data clusters\",len(train_features), \"Number of data points in trian data\", len(train_features[0]), \"Each data point contains\", len(train_features[0][0]),\"features\")\n",
    "print(\"Number of data clusters\",len(train_features), \"Number of data points in test data\", len(test_features[0]), \"Each data point contains\", len(test_features[0][0]),\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting first sizeof_train_data timestamp values i.e 70% of 13099 (total timestamps) for our training data\n",
    "\n",
    "tsne_train_flat_PULocationID = [i[:sizeof_train_data] for i in tsne_PULocationID]\n",
    "tsne_train_flat_Tip_amount = [i[:sizeof_train_data] for i in predict_list_tip]\n",
    "tsne_train_flat_weekday = [i[:sizeof_train_data] for i in tsne_weekday]\n",
    "tsne_train_flat_output = [i[:sizeof_train_data] for i in output]\n",
    "tsne_train_flat_output_tip = [i[:sizeof_train_data] for i in output_tip]\n",
    "tsne_train_flat_exp_avg = [i[:sizeof_train_data] for i in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the rest of the timestamp values i.e 30% of sizeof_train_data + sizeof_test_data (total timestamps) for our test data\n",
    "\n",
    "tsne_test_flat_PULocationID = [i[sizeof_train_data:] for i in tsne_PULocationID]\n",
    "tsne_test_flat_Tip_amount = [i[sizeof_train_data:] for i in predict_list_tip]\n",
    "tsne_test_flat_weekday = [i[sizeof_train_data:] for i in tsne_weekday]\n",
    "tsne_test_flat_output = [i[sizeof_train_data:] for i in output]\n",
    "tsne_test_flat_output_tip = [i[sizeof_train_data:] for i in output_tip]\n",
    "tsne_test_flat_exp_avg = [i[sizeof_train_data:] for i in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above contains values in the form of list of lists (i.e. list of values of each region), here we make all of them in one list\n",
    "train_new_features = []\n",
    "for i in range(0,nyc_regions_cnt):\n",
    "    train_new_features.extend(train_features[i])\n",
    "test_new_features = []\n",
    "for i in range(0,nyc_regions_cnt):\n",
    "    test_new_features.extend(test_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_train_PULocationID = sum(tsne_train_flat_PULocationID, [])\n",
    "tsne_train_Tip_amount = sum(tsne_train_flat_Tip_amount, [])\n",
    "tsne_train_weekday = sum(tsne_train_flat_weekday, [])\n",
    "tsne_train_output = sum(tsne_train_flat_output, [])\n",
    "tsne_train_output_tip = sum(tsne_train_flat_output_tip, [])\n",
    "tsne_train_exp_avg = sum(tsne_train_flat_exp_avg,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_test_PULocationID = sum(tsne_test_flat_PULocationID, [])\n",
    "tsne_test_Tip_amount = sum(tsne_test_flat_Tip_amount, [])\n",
    "tsne_test_weekday = sum(tsne_test_flat_weekday, [])\n",
    "tsne_test_output = sum(tsne_test_flat_output, [])\n",
    "tsne_test_output_tip = sum(tsne_test_flat_output_tip, [])\n",
    "tsne_test_exp_avg = sum(tsne_test_flat_exp_avg,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(841317, 9)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data frame for our train data\n",
    "columns = ['ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "df_train = pd.DataFrame(data=train_new_features, columns=columns) \n",
    "# df_train['lat'] = tsne_train_lat\n",
    "# df_train['lon'] = tsne_train_lon\n",
    "\n",
    "df_train['PULocationID'] = tsne_train_PULocationID\n",
    "df_train['Tip_amount'] = tsne_train_Tip_amount\n",
    "df_train['weekday'] = tsne_train_weekday\n",
    "df_train['exp_avg'] = tsne_train_exp_avg\n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360594, 9)\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data frame for our train data\n",
    "df_test = pd.DataFrame(data=test_new_features, columns=columns) \n",
    "# df_test['lat'] = tsne_test_lat\n",
    "# df_test['lon'] = tsne_test_lon\n",
    "\n",
    "df_test['PULocationID'] = tsne_test_PULocationID\n",
    "df_test['Tip_amount'] = tsne_test_Tip_amount\n",
    "df_test['weekday'] = tsne_test_weekday\n",
    "df_test['exp_avg'] = tsne_test_exp_avg\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find more about LinearRegression function here http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
    "\n",
    "# some of methods of LinearRegression()\n",
    "# fit(X, y[, sample_weight])\tFit linear model.\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict using the linear model\n",
    "# score(X, y[, sample_weight])\tReturns the coefficient of determination R^2 of the prediction.\n",
    "# set_params(**params)\tSet the parameters of this estimator.\n",
    "# -----------------------\n",
    "# video link: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/geometric-intuition-1-2-copy-8/\n",
    "# -----------------------\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_reg=LinearRegression().fit(df_train, tsne_train_output)\n",
    "\n",
    "y_pred = lr_reg.predict(df_test)\n",
    "lr_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = lr_reg.predict(df_train)\n",
    "lr_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_features=&#x27;sqrt&#x27;, min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_features=&#x27;sqrt&#x27;, min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_features='sqrt', min_samples_leaf=4,\n",
       "                      min_samples_split=3, n_estimators=40, n_jobs=-1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned random forest regressor on our train data\n",
    "# find more about LinearRegression function here http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion=’mse’, max_depth=None, min_samples_split=2, \n",
    "# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# apply(X)\tApply trees in the forest to X, return leaf indices.\n",
    "# decision_path(X)\tReturn the decision path in the forest\n",
    "# fit(X, y[, sample_weight])\tBuild a forest of trees from the training set (X, y).\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(X)\tPredict regression target for X.\n",
    "# score(X, y[, sample_weight])\tReturns the coefficient of determination R^2 of the prediction.\n",
    "# -----------------------\n",
    "# video link1: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/regression-using-decision-trees-2/\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "regr1 = RandomForestRegressor(max_features='sqrt',min_samples_leaf=4,min_samples_split=3,n_estimators=40, n_jobs=-1)\n",
    "regr1.fit(df_train, tsne_train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data using our trained random forest model \n",
    "\n",
    "# the models regr1 is already hyper parameter tuned\n",
    "# the parameters that we got above are found using grid search\n",
    "\n",
    "y_pred = regr1.predict(df_test)\n",
    "rndf_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = regr1.predict(df_train)\n",
    "rndf_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>trip_times</th>\n",
       "      <th>pickup_times</th>\n",
       "      <th>Speed</th>\n",
       "      <th>pickup_bins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4</td>\n",
       "      <td>246</td>\n",
       "      <td>26.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.583333</td>\n",
       "      <td>1.546269e+09</td>\n",
       "      <td>5.618070</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.70</td>\n",
       "      <td>4</td>\n",
       "      <td>87</td>\n",
       "      <td>13.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.583333</td>\n",
       "      <td>1.546269e+09</td>\n",
       "      <td>16.904348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>4</td>\n",
       "      <td>233</td>\n",
       "      <td>11.75</td>\n",
       "      <td>1.95</td>\n",
       "      <td>9.066667</td>\n",
       "      <td>1.546270e+09</td>\n",
       "      <td>15.220588</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4</td>\n",
       "      <td>263</td>\n",
       "      <td>20.15</td>\n",
       "      <td>3.35</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>1.546269e+09</td>\n",
       "      <td>20.287770</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.90</td>\n",
       "      <td>4</td>\n",
       "      <td>261</td>\n",
       "      <td>17.15</td>\n",
       "      <td>2.85</td>\n",
       "      <td>9.350000</td>\n",
       "      <td>1.546271e+09</td>\n",
       "      <td>25.026738</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950960</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.26</td>\n",
       "      <td>263</td>\n",
       "      <td>232</td>\n",
       "      <td>36.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.548916e+09</td>\n",
       "      <td>12.520000</td>\n",
       "      <td>4412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950961</th>\n",
       "      <td>NaN</td>\n",
       "      <td>15.34</td>\n",
       "      <td>263</td>\n",
       "      <td>98</td>\n",
       "      <td>62.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.548919e+09</td>\n",
       "      <td>21.404651</td>\n",
       "      <td>4418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950962</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.16</td>\n",
       "      <td>263</td>\n",
       "      <td>32</td>\n",
       "      <td>46.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.548926e+09</td>\n",
       "      <td>19.628571</td>\n",
       "      <td>4429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950963</th>\n",
       "      <td>NaN</td>\n",
       "      <td>18.53</td>\n",
       "      <td>263</td>\n",
       "      <td>191</td>\n",
       "      <td>76.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>38.400000</td>\n",
       "      <td>1.548928e+09</td>\n",
       "      <td>28.953125</td>\n",
       "      <td>4431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950964</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.43</td>\n",
       "      <td>263</td>\n",
       "      <td>159</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.633333</td>\n",
       "      <td>1.548937e+09</td>\n",
       "      <td>17.002132</td>\n",
       "      <td>4447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6870890 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         passenger_count  trip_distance  PULocationID  DOLocationID  \\\n",
       "0                    1.0           3.80             4           246   \n",
       "1                    1.0           2.70             4            87   \n",
       "2                    1.0           2.30             4           233   \n",
       "3                    1.0           4.70             4           263   \n",
       "4                    1.0           3.90             4           261   \n",
       "...                  ...            ...           ...           ...   \n",
       "6950960              NaN           6.26           263           232   \n",
       "6950961              NaN          15.34           263            98   \n",
       "6950962              NaN           9.16           263            32   \n",
       "6950963              NaN          18.53           263           191   \n",
       "6950964              NaN           4.43           263           159   \n",
       "\n",
       "         total_amount  tip_amount  trip_times  pickup_times      Speed  \\\n",
       "0               26.30        0.00   40.583333  1.546269e+09   5.618070   \n",
       "1               13.30        1.00    9.583333  1.546269e+09  16.904348   \n",
       "2               11.75        1.95    9.066667  1.546270e+09  15.220588   \n",
       "3               20.15        3.35   13.900000  1.546269e+09  20.287770   \n",
       "4               17.15        2.85    9.350000  1.546271e+09  25.026738   \n",
       "...               ...         ...         ...           ...        ...   \n",
       "6950960         36.00        0.00   30.000000  1.548916e+09  12.520000   \n",
       "6950961         62.70        0.00   43.000000  1.548919e+09  21.404651   \n",
       "6950962         46.50        0.00   28.000000  1.548926e+09  19.628571   \n",
       "6950963         76.00        0.00   38.400000  1.548928e+09  28.953125   \n",
       "6950964         29.00        0.00   15.633333  1.548937e+09  17.002132   \n",
       "\n",
       "         pickup_bins  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  2  \n",
       "3                  1  \n",
       "4                  4  \n",
       "...              ...  \n",
       "6950960         4412  \n",
       "6950961         4418  \n",
       "6950962         4429  \n",
       "6950963         4431  \n",
       "6950964         4447  \n",
       "\n",
       "[6870890 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "months_frame[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date-time: 2019-04-01 12:00:00\n",
      "unix_time =  1554087600.0\n",
      "bin =  13032\n",
      "target_month =  3\n",
      "target_bin =  4392\n",
      "weekday =  1\n"
     ]
    }
   ],
   "source": [
    "input_string = '2019-04-01 12:00:00'\n",
    "input_time = datetime.datetime.strptime(input_string, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print('Date-time:', input_time)\n",
    "\n",
    "\n",
    "def convert_to_unix(s):\n",
    "    return time.mktime(s.timetuple())\n",
    "\n",
    "start_pickup_unix = 1546268400 #local time 201901010000\n",
    "\n",
    "def calculate_bin(time):\n",
    "    return (int((time-start_pickup_unix)/600)) # !!!!!!!!!!!! 말해보자 -> 33(330분)을 더해주는 이유? [안 더하면 요일계산 잘됨]\n",
    "\n",
    "unix_time = convert_to_unix(input_time)\n",
    "print('unix_time = ', unix_time)\n",
    "\n",
    "bin = calculate_bin(unix_time)\n",
    "print('bin = ', bin)\n",
    "\n",
    "def calculate_month(bin):\n",
    "    month = 0\n",
    "    check = 0\n",
    "    while bin>0:\n",
    "        bin -= pickup_bins_len[check]\n",
    "        check+=1\n",
    "        month+=1\n",
    "    return month\n",
    "\n",
    "## 입력받은 날짜의 월 계산\n",
    "target_month = calculate_month(bin)\n",
    "print(\"target_month = \", target_month)\n",
    "\n",
    "## 입력받은 날짜의 월에 해당하는 bin 구하기\n",
    "target_bin = bin\n",
    "for i in range(target_month-1):\n",
    "    target_bin -= pickup_bins_len[i]\n",
    "\n",
    "print(\"target_bin = \", target_bin)\n",
    "\n",
    "weekday = ((int(bin/144))%7+day_of_the_week_dict[base_year+1])%7\n",
    "print(\"weekday = \", weekday)\n",
    "\n",
    "# exp_avg_test = predict_list[3][bin] # predict_list[n][bin] -> n-1번 지역의 bin에서의 exp_avg\n",
    "# print(\"exp_avg =\", exp_avg_test)\n",
    "\n",
    "before_frame = months_frame[target_month - 1][['PULocationID','pickup_bins']] #target_frame = 예측년도를 전처리 한 프레임 (jan_base_year_frame)\n",
    "after_frame = before_frame.loc[(before_frame['pickup_bins'] <target_bin) & (before_frame['pickup_bins']>=target_bin-5)]\n",
    "\n",
    "# for i in range(1, 2): ## 1~5번째 전 bin의 탑승 수를 맨해튼 모든 지역별로 돌려서 저장 \n",
    "# ft_1이 10분 전\n",
    "\n",
    "input_list = []\n",
    "\n",
    "columns = ['ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "temp = 0 \n",
    "for i in nyc_region_number.values:\n",
    "    temp_list = [0 for i in range(5)]\n",
    "    temp1_frame = after_frame.loc[(after_frame['PULocationID']==i)]\n",
    "    globals()['region_{}_frame'.format(i)] = pd.DataFrame(columns = columns)\n",
    "    for j in range(1,6):\n",
    "        temp2_frame = temp1_frame.loc[(temp1_frame['pickup_bins'] == target_bin-j)] # ft_1, ft_2, ft_3, ft_4, ft_5 구하기\n",
    "        temp_list[5-j] = len(temp2_frame)\n",
    "        # globals()['region_{}_bins_{}'.format(i, j)] = len(temp2_frame)\n",
    "    eval('region_' + str(i) + '_frame').loc[str(i)+ '번지역'] = temp_list\n",
    "    eval('region_' + str(i) + '_frame')['weekday'] = weekday\n",
    "    eval('region_' + str(i) + '_frame')['exp_avg'] = predict_list[temp][bin]\n",
    "    eval('region_' + str(i) + '_frame')['PULocationID'] = i\n",
    "    input_list.append(eval('region_' + str(i) + '_frame'))\n",
    "    temp += 1\n",
    "\n",
    "predict_df = pd.DataFrame()\n",
    "region_list = []\n",
    "predict_number_list = []\n",
    "\n",
    "# jinsu_index = 0\n",
    "# for i in input_list:\n",
    "#     region_list.append(i['PULocationID'].iloc[0])\n",
    "#     predict_number_list.append(regr1.predict(i).item())\n",
    "\n",
    "# predict_df['OBJECTID'] = region_list\n",
    "# predict_df['predict_number'] = predict_number_list\n",
    "\n",
    "# print(predict_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft_5</th>\n",
       "      <th>ft_4</th>\n",
       "      <th>ft_3</th>\n",
       "      <th>ft_2</th>\n",
       "      <th>ft_1</th>\n",
       "      <th>weekday</th>\n",
       "      <th>exp_avg</th>\n",
       "      <th>PULocationID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4번지역</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ft_5  ft_4  ft_3  ft_2  ft_1  weekday  exp_avg  PULocationID\n",
       "4번지역     6     5     1     7     2        1        2             4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_4_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ft_5', 'ft_4', 'ft_3', 'ft_2', 'ft_1', 'PULocationID', 'Tip_amount',\n",
      "       'weekday', 'exp_avg'],\n",
      "      dtype='object')\n",
      "[0.02702976 0.07878971 0.12398314 0.20980948 0.24890181 0.00454074\n",
      " 0.01158774 0.0022948  0.29306281]\n"
     ]
    }
   ],
   "source": [
    "#feature importances based on analysis using random forest\n",
    "print (df_train.columns)\n",
    "print (regr1.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using XgBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "             random_state=0, reg_alpha=200, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "             importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "             random_state=0, reg_alpha=200, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=3,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=1000,\n",
       "             n_jobs=4, nthread=4, num_parallel_tree=1, predictor='auto',\n",
       "             random_state=0, reg_alpha=200, ...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a hyper-parameter tuned Xg-Boost regressor on our train data\n",
    "\n",
    "# find more about XGBRegressor function here http://xgboost.readthedocs.io/en/latest/python/python_api.html?#module-xgboost.sklearn\n",
    "# -------------------------\n",
    "# default paramters\n",
    "# xgboost.XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', \n",
    "# booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, \n",
    "# colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, \n",
    "# missing=None, **kwargs)\n",
    "\n",
    "# some of methods of RandomForestRegressor()\n",
    "# fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
    "# get_params([deep])\tGet parameters for this estimator.\n",
    "# predict(data, output_margin=False, ntree_limit=0) : Predict with data. NOTE: This function is not thread safe.\n",
    "# get_score(importance_type='weight') -> get the feature importance\n",
    "# -----------------------\n",
    "# video link1: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/regression-using-decision-trees-2/\n",
    "# video link2: https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/what-are-ensembles/\n",
    "# -----------------------\n",
    "\n",
    "x_model = xgb.XGBRegressor(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=3,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " reg_alpha=200, reg_lambda=200,\n",
    " colsample_bytree=0.8,nthread=4)\n",
    "x_model.fit(df_train, tsne_train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting with our trained Xg-Boost regressor\n",
    "# the models x_model is already hyper parameter tuned\n",
    "# the parameters that we got above are found using grid search\n",
    "\n",
    "y_pred = x_model.predict(df_test)\n",
    "xgb_test_predictions = [round(value) for value in y_pred]\n",
    "y_pred = x_model.predict(df_train)\n",
    "xgb_train_predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ft_5': 871.0,\n",
       " 'ft_4': 641.0,\n",
       " 'ft_3': 759.0,\n",
       " 'ft_2': 911.0,\n",
       " 'ft_1': 1202.0,\n",
       " 'PULocationID': 947.0,\n",
       " 'Tip_amount': 696.0,\n",
       " 'weekday': 166.0,\n",
       " 'exp_avg': 733.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature importances\n",
    "x_model.get_booster().get_score(importance_type=\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the error metric values for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_R2=[]\n",
    "test_R2=[]\n",
    "\n",
    "train_R2.append(r2_score(tsne_train_output,df_train['ft_1'].values))\n",
    "train_R2.append(r2_score(tsne_train_output,df_train['exp_avg'].values))\n",
    "train_R2.append(r2_score(tsne_train_output,rndf_train_predictions))\n",
    "train_R2.append(r2_score(tsne_train_output,xgb_train_predictions))\n",
    "train_R2.append(r2_score(tsne_train_output,lr_train_predictions))\n",
    "train_R2.append(r2_score(tsne_train_output_tip,df_train['Tip_amount'].values))\n",
    "\n",
    "test_R2.append(r2_score(tsne_test_output,df_test['ft_1'].values))\n",
    "test_R2.append(r2_score(tsne_test_output,df_test['exp_avg'].values))\n",
    "test_R2.append(r2_score(tsne_test_output,rndf_test_predictions))\n",
    "test_R2.append(r2_score(tsne_test_output,xgb_test_predictions))\n",
    "test_R2.append(r2_score(tsne_test_output,lr_test_predictions))\n",
    "test_R2.append(r2_score(tsne_test_output_tip,df_test['Tip_amount'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 1 ~ 4 Manhattan R2\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Baseline Model\t\t\t\tTrain: \t0.9301406638166664\tTest: \t0.9287072588080383\n",
      "Exponential Averages Forecasting\tTrain: \t0.9383407215868186\tTest: \t0.9369626701151025\n",
      "Random Forest Regression\t\tTrain: \t0.9705878846632486\tTest: \t0.940160351321223\n",
      "XgBoost Regression\t\t\tTrain: \t0.9438444420463238\tTest: \t0.9411161918214299\n",
      "Linear Regression\t\t\tTrain: \t0.9398156609879582\tTest: \t0.9384612705131654\n",
      "Baseline Model of Tip\t\t\tTrain: \t0.08435530222445842\tTest: \t0.09624857667441022\n"
     ]
    }
   ],
   "source": [
    "print(f\"{base_year+1} 1 ~ {base_month_count} {region} R2\")\n",
    "print (\"--------------------------------------------------------------------------------------------------------\")\n",
    "print (\"Baseline Model\\t\\t\\t\",\"Train: \",train_R2[0],\"Test: \",test_R2[0],sep='\\t')\n",
    "print (\"Exponential Averages Forecasting\",\"Train: \",train_R2[1],\"Test: \",test_R2[1],sep='\\t')\n",
    "print (\"Random Forest Regression\\t\",\"Train: \",train_R2[2],\"Test: \",test_R2[2],sep='\\t')\n",
    "print (\"XgBoost Regression\\t\\t\",\"Train: \",train_R2[3],\"Test: \",test_R2[3],sep='\\t')\n",
    "print (\"Linear Regression\\t\\t\",\"Train: \",train_R2[4],\"Test: \",test_R2[4],sep='\\t')\n",
    "print (\"Baseline Model of Tip\\t\\t\",\"Train: \",train_R2[5],\"Test: \",test_R2[5],sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MAE=[]\n",
    "test_MAE=[]\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,df_train['ft_1'].values))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,df_train['exp_avg'].values))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,rndf_train_predictions))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,xgb_train_predictions))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output,lr_train_predictions))\n",
    "train_MAE.append(mean_absolute_error(tsne_train_output_tip,df_train['Tip_amount'].values))\n",
    "\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,df_test['ft_1'].values))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,df_test['exp_avg'].values))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,rndf_test_predictions))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,xgb_test_predictions))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output,lr_test_predictions))\n",
    "test_MAE.append(mean_absolute_error(tsne_test_output_tip,df_test['Tip_amount'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 1 ~ 4 Manhattan MAE\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Baseline Model\t\t\t\tTrain: \t4.302377106370131\tTest: \t4.294098071515333\n",
      "Exponential Averages Forecasting\tTrain: \t4.0275294567921485\tTest: \t4.018067410994082\n",
      "Random Forest Regression\t\tTrain: \t2.7733220652857367\tTest: \t3.9659977703455964\n",
      "XgBoost Regression\t\t\tTrain: \t3.906025909377797\tTest: \t3.9271979012407305\n",
      "Linear Regression\t\t\tTrain: \t3.9945811150850394\tTest: \t3.990077483263726\n",
      "Baseline Model of Tip\t\t\tTrain: \t0.46200474572520744\tTest: \t0.4986594814925792\n"
     ]
    }
   ],
   "source": [
    "print(f\"{base_year+1} 1 ~ {base_month_count} {region} MAE\")\n",
    "print (\"--------------------------------------------------------------------------------------------------------\")\n",
    "print (\"Baseline Model\\t\\t\\t\",\"Train: \",train_MAE[0],\"Test: \",test_MAE[0],sep='\\t')\n",
    "print (\"Exponential Averages Forecasting\",\"Train: \",train_MAE[1],\"Test: \",test_MAE[1],sep='\\t')\n",
    "print (\"Random Forest Regression\\t\",\"Train: \",train_MAE[2],\"Test: \",test_MAE[2],sep='\\t')\n",
    "print (\"XgBoost Regression\\t\\t\",\"Train: \",train_MAE[3],\"Test: \",test_MAE[3],sep='\\t')\n",
    "print (\"Linear Regression\\t\\t\",\"Train: \",train_MAE[4],\"Test: \",test_MAE[4],sep='\\t')\n",
    "print (\"Baseline Model of Tip\\t\\t\",\"Train: \",train_MAE[5],\"Test: \",test_MAE[5],sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
